# PRODUCTION DEPLOYMENT GUIDE
## Whisper Encoder on AMD XDNA2 NPU

**Version**: 1.0 (BFP16)
**Date**: October 30, 2025
**Status**: For use after BFP16 migration complete
**Target Hardware**: AMD Strix Halo (XDNA2 NPU)

---

## Table of Contents

1. [System Requirements](#system-requirements)
2. [Installation](#installation)
3. [Pre-Warming Strategy](#pre-warming-strategy)
4. [Performance Expectations](#performance-expectations)
5. [Monitoring and SLAs](#monitoring-and-slas)
6. [Troubleshooting Guide](#troubleshooting-guide)
7. [Deployment Checklist](#deployment-checklist)
8. [Configuration Options](#configuration-options)

---

## System Requirements

### Hardware Requirements

**Minimum** (for 17√ó realtime):
- AMD Ryzen AI MAX+ 395 (Strix Halo)
- XDNA2 NPU (32 tiles, 50 TOPS)
- 200 MB available RAM
- 500 MB available storage (for weights and kernels)

**Recommended** (for 20√ó realtime):
- Same as minimum
- 4 GB available RAM (for caching and warm-up)
- 2 GB available storage (for multiple models)

**Verified Working**:
- ASUS ROG Flow Z13 GZ302EA
- AMD Ryzen AI MAX+ 395 (16C/32T)
- XDNA2 NPU (32 tiles, 50 TOPS)
- 120 GB RAM (only 200 MB used)

### Software Requirements

**Required**:
- Ubuntu Server 25.10+ (or compatible Linux)
- Kernel: 6.17.0+ (for XDNA2 driver support)
- XRT 2.21.0+ (AMD XDNA Runtime)
- Python 3.13+
- C++17 compiler (GCC 11+ or Clang 14+)

**Python Dependencies**:
```bash
numpy>=1.24.0
torch>=2.0.0 (for weight loading only, not inference)
Eigen3>=3.4.0 (C++ library)
```

**System Libraries**:
```bash
libc6>=2.35
libstdc++6>=11.0
libgomp1 (for OpenMP support)
```

### Disk Space Requirements

```
Whisper Base Model:
  FP32 weights:       75 MB
  BFP16 weights:      64 MB (9-bit encoding)
  XCLBin kernel:      15 MB
  C++ library:        2 MB
  Total:              ~160 MB

Temporary Files:
  Warm-up cache:      50 MB
  Logs:               10 MB
  Total:              ~60 MB

Total Required:       ~220 MB
```

---

## Installation

### Step 1: Install System Dependencies

```bash
# Update package list
sudo apt update

# Install XRT (AMD XDNA Runtime)
wget https://www.xilinx.com/bin/public/openDownload?filename=xrt_202420.2.18.286_25.10-amd64-xrt.deb
sudo dpkg -i xrt_202420.2.18.286_25.10-amd64-xrt.deb
sudo apt install -f

# Verify XRT installation
xbutil examine

# Install build dependencies
sudo apt install -y \
  build-essential \
  cmake \
  libeigen3-dev \
  libgomp1 \
  python3-dev \
  python3-pip
```

### Step 2: Install Python Dependencies

```bash
# Create virtual environment (recommended)
python3 -m venv /opt/unicorn-amanuensis
source /opt/unicorn-amanuensis/bin/activate

# Install Python packages
pip install numpy torch

# Verify installation
python3 -c "import numpy; import torch; print('OK')"
```

### Step 3: Install Whisper Encoder

```bash
# Clone repository
cd /opt
git clone https://github.com/Unicorn-Commander/unicorn-amanuensis.git
cd unicorn-amanuensis/xdna2

# Build C++ library
mkdir -p cpp/build
cd cpp/build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)

# Verify build
ls -lh libwhisper_encoder_cpp.so

# Install library
sudo cp libwhisper_encoder_cpp.so /usr/local/lib/
sudo ldconfig
```

### Step 4: Download Weights

```bash
# Download OpenAI Whisper Base weights
cd /opt/unicorn-amanuensis/xdna2
python3 download_whisper_weights.py

# Convert to BFP16 (after BFP16 migration complete)
python3 convert_weights_to_bfp16.py

# Verify weights
ls -lh weights/
# Expected:
# - whisper_base_encoder_real_fp32.npz (75 MB)
# - whisper_base_encoder_real_bfp16.npz (64 MB)
```

### Step 5: Install XCLBin Kernel

```bash
# Copy BFP16 kernel to system location
sudo mkdir -p /opt/xilinx/xclbin
sudo cp kernels/matmul_32tile_bfp16.xclbin /opt/xilinx/xclbin/

# Set environment variable
echo 'export XCLBIN_PATH=/opt/xilinx/xclbin' | sudo tee -a /etc/environment
source /etc/environment

# Verify kernel
ls -lh /opt/xilinx/xclbin/matmul_32tile_bfp16.xclbin
```

### Step 6: Verify Installation

```bash
# Run basic test
cd /opt/unicorn-amanuensis/xdna2
python3 test_basic.py

# Expected output:
# ‚úÖ C++ library loaded
# ‚úÖ Weights loaded (97 tensors)
# ‚úÖ NPU kernel loaded
# ‚úÖ Basic inference: 550ms (18.6√ó realtime)
# Status: READY
```

---

## Pre-Warming Strategy

### Why Pre-Warming is Critical

Performance improves by **17.5%** after warm-up:
- Cold start: 600-640ms (16-17√ó realtime)
- Steady-state: 510-570ms (18-20√ó realtime)
- Improvement: -90ms to -130ms per inference

**Warm-up causes**:
- NPU kernel compilation/optimization
- CPU cache warming
- Memory allocation patterns stabilizing
- System scheduler learning

### Recommended Pre-Warming Approach

**Option 1: On Application Startup** (Recommended)

```python
#!/usr/bin/env python3
"""
Pre-warm the encoder during application startup.
Run this once when your service starts.
"""
import numpy as np
from encoder import WhisperEncoder

def warmup_encoder(iterations=100):
    """
    Warm up the encoder by running multiple iterations.

    Args:
        iterations: Number of warm-up iterations (default: 100)
                   - 50: Minimal warm-up (~25 seconds)
                   - 100: Recommended (~50 seconds)
                   - 200: Maximum (~100 seconds)
    """
    print(f"Warming up encoder ({iterations} iterations)...")

    # Initialize encoder
    encoder = WhisperEncoder(
        model="base",
        weights_path="/opt/unicorn-amanuensis/xdna2/weights/whisper_base_encoder_real_bfp16.npz",
        xclbin_path="/opt/xilinx/xclbin/matmul_32tile_bfp16.xclbin"
    )

    # Create dummy input (same shape as real input)
    dummy_input = np.random.randn(512, 512).astype(np.float32)

    # Warm-up loop
    for i in range(iterations):
        _ = encoder.encode(dummy_input)
        if (i + 1) % 10 == 0:
            print(f"  Progress: {i + 1}/{iterations}")

    print(f"‚úÖ Warm-up complete! Encoder ready for production.")
    return encoder

# Run on startup
if __name__ == "__main__":
    encoder = warmup_encoder(iterations=100)
    # Now use encoder for real inference (will be 18-20√ó realtime)
```

**Option 2: Systemd Service Pre-Warming**

```ini
# /etc/systemd/system/unicorn-amanuensis-warmup.service
[Unit]
Description=Unicorn Amanuensis Encoder Warm-up
After=network.target

[Service]
Type=oneshot
User=unicorn
Group=unicorn
WorkingDirectory=/opt/unicorn-amanuensis/xdna2
ExecStart=/opt/unicorn-amanuensis/bin/python3 warmup_encoder.py
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start warm-up service
sudo systemctl enable unicorn-amanuensis-warmup.service
sudo systemctl start unicorn-amanuensis-warmup.service

# Check status
sudo systemctl status unicorn-amanuensis-warmup.service
```

**Option 3: Docker Container Pre-Warming**

```dockerfile
# Dockerfile with pre-warming
FROM ubuntu:25.10

# Install dependencies (as above)
# ...

# Copy encoder
COPY . /opt/unicorn-amanuensis

# Pre-warm during container build (optional)
RUN python3 /opt/unicorn-amanuensis/xdna2/warmup_encoder.py --iterations=50

# Or pre-warm on container start (recommended)
CMD ["python3", "/opt/unicorn-amanuensis/xdna2/warmup_and_serve.py"]
```

### Pre-Warming Parameters

| Iterations | Time | Steady-State Reached | Recommended For |
|-----------|------|---------------------|----------------|
| **50** | ~25s | 90% | Development, testing |
| **100** | ~50s | 95% | Production (recommended) |
| **200** | ~100s | 99%+ | Mission-critical, high-reliability |

**Production Recommendation**: 100 iterations (~50 seconds one-time startup cost)

---

## Performance Expectations

### Expected Performance (BFP16)

**Cold Start** (no warm-up):
```
Average Time:      600-640 ms
Realtime Factor:   16-17√ó
Consistency:       99.7%
Use Case:          One-off transcriptions
```

**Warm Start** (after 100 iterations):
```
Average Time:      510-570 ms
Realtime Factor:   18-20√ó ‚≠ê
Consistency:       99.2%+
Use Case:          Production services
```

**Peak Performance**:
```
Best Time:         430-470 ms
Realtime Factor:   22-24√ó
Consistency:       N/A (peak only)
Use Case:          Ideal conditions
```

### Performance by Audio Duration

| Audio Duration | Cold Start | Warm Start | Target |
|---------------|-----------|-----------|--------|
| 10s | 600-640ms | 510-570ms | <1000ms ‚úÖ |
| 30s | 1800-1920ms | 1530-1710ms | <3000ms ‚úÖ |
| 60s | 3600-3840ms | 3060-3420ms | <6000ms ‚úÖ |

**Realtime Definition**: 1√ó realtime = process 1 second of audio in 1 second
**Our Performance**: 18-20√ó realtime = process 1 second of audio in 50-55ms

### Accuracy Expectations (BFP16)

```
Cosine Similarity:     >99% (vs PyTorch reference)
Mean Absolute Error:   <0.01
Max Absolute Error:    <1.0
Element Accuracy:      >99%
```

### Power and Battery

**Power Draw**:
- Idle: 0W (NPU off)
- Inference: 5-15W (NPU active)
- Average: 8W (typical workload)

**Battery Life** (52 Wh battery):
- Continuous inference: 6+ hours
- Mixed usage (50% inference): 12+ hours
- Idle: 18+ hours

**Thermal**:
- NPU temperature: 45-60¬∞C (passive cooling)
- No thermal throttling observed
- Fan noise: Minimal to none

---

## Monitoring and SLAs

### Key Metrics to Monitor

**1. Performance Metrics**
```python
# Track these metrics in production
metrics = {
    "inference_time_ms": 550,           # Should be 510-570ms
    "realtime_factor": 18.6,            # Should be 18-20√ó
    "throughput_requests_per_min": 100, # Depends on audio length
    "queue_depth": 5,                   # Should be <10
}
```

**2. Accuracy Metrics**
```python
# Periodically validate accuracy (daily or weekly)
accuracy_metrics = {
    "cosine_similarity": 0.995,  # Should be >0.99
    "wer": 0.05,                 # Word Error Rate <5%
    "successful_transcriptions": 0.998,  # >99.5%
}
```

**3. Resource Metrics**
```python
# Monitor system resources
resource_metrics = {
    "npu_utilization": 0.023,    # 2.3% (very low, headroom)
    "memory_usage_mb": 200,      # Should be <300 MB
    "cpu_usage": 0.15,           # 15% (for pre/post processing)
    "power_draw_w": 8,           # Should be 5-15W
}
```

**4. Reliability Metrics**
```python
# Track errors and failures
reliability_metrics = {
    "error_rate": 0.0,           # Should be 0%
    "npu_errors": 0,             # Should be 0
    "numerical_issues": 0,       # No NaN/Inf
    "uptime_hours": 720,         # 30 days
}
```

### Recommended SLAs

**Tier 1: Performance SLA**
```
Metric:              Inference Time
Target:              <600ms (95th percentile)
Threshold:           <700ms (99th percentile)
Action if exceeded:  Investigate system load, check warm-up
```

**Tier 2: Accuracy SLA**
```
Metric:              Cosine Similarity
Target:              >99%
Threshold:           >98%
Action if exceeded:  Validate weights, check BFP16 quantization
```

**Tier 3: Reliability SLA**
```
Metric:              Error Rate
Target:              0% (zero errors)
Threshold:           <0.1%
Action if exceeded:  Check NPU health, review logs
```

**Tier 4: Availability SLA**
```
Metric:              Uptime
Target:              99.9% (8.76 hours downtime/year)
Threshold:           99.5%
Action if exceeded:  Investigate crashes, resource exhaustion
```

### Monitoring Tools

**Prometheus + Grafana** (Recommended):

```python
# Export metrics to Prometheus
from prometheus_client import Histogram, Counter, Gauge

# Define metrics
inference_time = Histogram(
    'whisper_inference_time_seconds',
    'Time to process audio (seconds)',
    buckets=[0.4, 0.5, 0.6, 0.7, 0.8, 1.0]
)

accuracy = Gauge(
    'whisper_cosine_similarity',
    'Cosine similarity vs reference'
)

errors = Counter(
    'whisper_errors_total',
    'Total number of errors'
)

# Instrument your code
with inference_time.time():
    result = encoder.encode(audio)

accuracy.set(compute_accuracy(result, reference))
```

**Grafana Dashboard** (example queries):

```promql
# 95th percentile inference time
histogram_quantile(0.95, rate(whisper_inference_time_seconds_bucket[5m]))

# Error rate (errors per second)
rate(whisper_errors_total[5m])

# Accuracy trend
avg_over_time(whisper_cosine_similarity[1h])
```

---

## Troubleshooting Guide

### Issue 1: Slow Performance (<17√ó realtime)

**Symptoms**:
- Inference time >600ms
- Realtime factor <17√ó
- Inconsistent timings

**Possible Causes**:
1. No warm-up performed
2. System under heavy load
3. NPU kernel not loaded
4. Using wrong weights (FP32 instead of BFP16)

**Solutions**:
```bash
# 1. Check if warm-up was performed
python3 -c "import warmup; warmup.check_status()"

# 2. Check system load
top
# Look for high CPU usage by other processes

# 3. Verify NPU kernel
xbutil examine | grep -i xclbin
# Should show matmul_32tile_bfp16.xclbin

# 4. Verify weights format
python3 -c "import numpy as np; w = np.load('weights/whisper_base_encoder_real_bfp16.npz'); print(w.files)"
# Should show BFP16 format

# 5. Force warm-up
python3 warmup_encoder.py --iterations=100
```

### Issue 2: Low Accuracy (<99%)

**Symptoms**:
- Cosine similarity <0.99
- High word error rate
- Garbled transcriptions

**Possible Causes**:
1. Using INT8 weights instead of BFP16
2. Weights corrupted or not loaded
3. BFP16 conversion error

**Solutions**:
```bash
# 1. Verify weights format
python3 test_weights_format.py
# Should show "BFP16" not "INT8"

# 2. Re-download weights
rm weights/*.npz
python3 download_whisper_weights.py
python3 convert_weights_to_bfp16.py

# 3. Validate accuracy
python3 test_accuracy_vs_pytorch.py
# Should show >99% cosine similarity

# 4. Check BFP16 conversion
python3 validate_bfp16_conversion.py
# Should show <1% error vs FP32
```

### Issue 3: NPU Errors

**Symptoms**:
- "NPU execution failed" errors
- Zero output or NaN values
- Crashes during inference

**Possible Causes**:
1. XRT not properly installed
2. XCLBin kernel not found
3. NPU device not detected
4. Memory exhaustion

**Solutions**:
```bash
# 1. Check XRT installation
xbutil examine
# Should show XDNA2 device

# 2. Verify XCLBin path
echo $XCLBIN_PATH
ls -lh $XCLBIN_PATH/matmul_32tile_bfp16.xclbin

# 3. Check NPU device
ls -la /dev/dri/
# Should show renderD128 or similar

# 4. Check memory
free -h
# Should have >200 MB available

# 5. Restart XRT service
sudo systemctl restart xrt
```

### Issue 4: Memory Leaks

**Symptoms**:
- Memory usage increases over time
- System becomes slow after many inferences
- Out-of-memory errors

**Possible Causes**:
1. Not freeing encoder resources
2. Accumulating intermediate tensors
3. NPU buffer not released

**Solutions**:
```python
# Proper resource management
def run_inference(audio):
    encoder = WhisperEncoder()  # Create encoder
    try:
        result = encoder.encode(audio)
        return result
    finally:
        encoder.cleanup()  # Release resources

# Or use context manager (recommended)
with WhisperEncoder() as encoder:
    result = encoder.encode(audio)
# Resources automatically released
```

```bash
# Monitor memory usage
watch -n 1 'free -h'

# Check for leaks
valgrind --leak-check=full python3 test_inference.py
```

### Issue 5: Inconsistent Performance

**Symptoms**:
- Wide variance in inference time
- Occasional slow inferences (>1000ms)
- Unpredictable timing

**Possible Causes**:
1. System power saving mode
2. Thermal throttling
3. Background processes
4. Insufficient warm-up

**Solutions**:
```bash
# 1. Disable power saving
sudo cpupower frequency-set -g performance

# 2. Check thermals
sensors | grep -i temp
# NPU should be <70¬∞C

# 3. Identify background processes
ps aux --sort=-%cpu | head -20

# 4. Increase warm-up iterations
python3 warmup_encoder.py --iterations=200

# 5. Set process priority
nice -n -10 python3 your_service.py
```

---

## Deployment Checklist

### Pre-Deployment

- [ ] Hardware verified (AMD Strix Halo, XDNA2 NPU)
- [ ] XRT 2.21.0+ installed
- [ ] C++ library built and installed
- [ ] Python dependencies installed
- [ ] Weights downloaded (BFP16 format)
- [ ] XCLBin kernel installed
- [ ] Basic test passed
- [ ] Accuracy test passed (>99%)
- [ ] Performance test passed (18-20√ó)

### Deployment

- [ ] Pre-warming implemented (100 iterations)
- [ ] Monitoring configured (Prometheus/Grafana)
- [ ] Logging configured (structured logs)
- [ ] Error handling implemented
- [ ] Resource limits set
- [ ] Systemd service created
- [ ] Auto-restart configured
- [ ] Health checks implemented

### Post-Deployment

- [ ] Verify performance (18-20√ó realtime)
- [ ] Verify accuracy (>99%)
- [ ] Monitor for 24 hours
- [ ] Check for errors (should be 0)
- [ ] Validate battery life (6+ hours)
- [ ] Load test (100+ concurrent requests)
- [ ] Stress test (1000+ sequential inferences)
- [ ] Documentation updated

---

## Configuration Options

### Encoder Configuration

```python
from encoder import WhisperEncoder

# Default configuration (recommended)
encoder = WhisperEncoder(
    model="base",                    # Model size (base, small, medium)
    weights_path="/opt/unicorn-amanuensis/xdna2/weights/whisper_base_encoder_real_bfp16.npz",
    xclbin_path="/opt/xilinx/xclbin/matmul_32tile_bfp16.xclbin",
    device="npu",                    # Device (npu, cpu, auto)
    warmup_iterations=100,           # Pre-warm on init (0 to disable)
    cache_weights=True,              # Cache weights in memory
    enable_logging=True,             # Enable performance logging
    log_level="INFO",                # Log level (DEBUG, INFO, WARN, ERROR)
)

# Performance tuning
encoder.set_performance_mode("balanced")  # balanced, power_save, max_performance

# Accuracy tuning
encoder.set_precision("bfp16")  # bfp16, fp32 (fallback)
```

### Environment Variables

```bash
# XCLBin kernel path
export XCLBIN_PATH=/opt/xilinx/xclbin

# Weights directory
export WHISPER_WEIGHTS_DIR=/opt/unicorn-amanuensis/xdna2/weights

# Log level
export WHISPER_LOG_LEVEL=INFO

# Performance mode
export WHISPER_PERFORMANCE_MODE=balanced

# Disable warm-up (not recommended)
export WHISPER_DISABLE_WARMUP=0

# Cache directory
export WHISPER_CACHE_DIR=/tmp/whisper_cache
```

### Systemd Service Configuration

```ini
# /etc/systemd/system/unicorn-amanuensis.service
[Unit]
Description=Unicorn Amanuensis Speech-to-Text Service
After=network.target xrt.service

[Service]
Type=simple
User=unicorn
Group=unicorn
WorkingDirectory=/opt/unicorn-amanuensis/xdna2

# Environment
Environment="XCLBIN_PATH=/opt/xilinx/xclbin"
Environment="WHISPER_WEIGHTS_DIR=/opt/unicorn-amanuensis/xdna2/weights"
Environment="WHISPER_LOG_LEVEL=INFO"

# Resource limits
MemoryLimit=1G
CPUQuota=200%

# Restart policy
Restart=on-failure
RestartSec=5s

# Execute
ExecStart=/opt/unicorn-amanuensis/bin/python3 serve.py

[Install]
WantedBy=multi-user.target
```

---

## Security Considerations

### File Permissions

```bash
# Set proper ownership
sudo chown -R unicorn:unicorn /opt/unicorn-amanuensis
sudo chmod -R 755 /opt/unicorn-amanuensis

# Protect weights (read-only)
sudo chmod 444 /opt/unicorn-amanuensis/xdna2/weights/*.npz

# Protect XCLBin (read-only)
sudo chmod 444 /opt/xilinx/xclbin/*.xclbin
```

### Network Security

- Run service on localhost only (127.0.0.1)
- Use TLS/SSL for external access
- Implement API key authentication
- Rate limit requests (e.g., 100/min per IP)

### Resource Limits

```bash
# Prevent resource exhaustion
ulimit -v 1048576    # 1 GB virtual memory
ulimit -m 524288     # 512 MB resident memory
ulimit -n 1024       # 1024 open files
```

---

## Performance Tuning

### System-Level Tuning

```bash
# 1. CPU governor (performance mode)
sudo cpupower frequency-set -g performance

# 2. Disable CPU sleep states
sudo cpupower idle-set -D 0

# 3. Increase swap (if needed)
sudo fallocate -l 4G /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 4. Optimize I/O scheduler
echo mq-deadline | sudo tee /sys/block/nvme0n1/queue/scheduler
```

### Application-Level Tuning

```python
# 1. Pre-allocate buffers
encoder = WhisperEncoder(preallocate_buffers=True)

# 2. Batch multiple requests
results = encoder.encode_batch([audio1, audio2, audio3])

# 3. Use async API
async with WhisperEncoder() as encoder:
    result = await encoder.encode_async(audio)

# 4. Enable caching
encoder.enable_cache(max_size=100)  # Cache last 100 results
```

---

## Contact and Support

**Project**: Unicorn Amanuensis
**Company**: Magic Unicorn Unconventional Technology & Stuff Inc
**GitHub**: https://github.com/Unicorn-Commander/unicorn-amanuensis
**Issues**: https://github.com/Unicorn-Commander/unicorn-amanuensis/issues
**Email**: support@magicunicorn.tech

**Documentation**:
- Installation Guide: `/opt/unicorn-amanuensis/xdna2/README.md`
- API Reference: `/opt/unicorn-amanuensis/xdna2/API.md`
- BFP16 Migration: `/opt/unicorn-amanuensis/xdna2/BFP16_INTEGRATION_ROADMAP.md`

---

**Version**: 1.0
**Last Updated**: October 30, 2025
**Status**: For use after BFP16 migration complete
**Expected Availability**: November 2025 (2 weeks)

**Built with üí™ by Team BRO**
**Powered by AMD XDNA2 NPU (32 tiles, 50 TOPS)**
