#!/usr/bin/env python3
"""
Quantize OpenVINO Whisper models to INT8 for maximum Intel iGPU performance
INT8 provides 2-4x better performance than FP16 on Intel integrated graphics
"""

import os
import sys
import shutil
import numpy as np
from pathlib import Path
import openvino as ov
from openvino.runtime import Core
import nncf
import logging
import argparse
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def quantize_openvino_model_int8(model_path: str, output_path: str, calibration_samples: int = 100):
    """
    Quantize OpenVINO model to INT8 using NNCF (Neural Network Compression Framework)
    
    INT8 benefits for Intel iGPU:
    - 4x smaller model size
    - 2-4x faster inference
    - Lower memory bandwidth requirements
    - Better cache utilization
    """
    
    logger.info(f"ðŸŽ¯ Quantizing {model_path} to INT8...")
    
    # Initialize OpenVINO runtime
    core = Core()
    
    # Load the FP16/FP32 model
    model_xml = Path(model_path)
    model = core.read_model(model_xml)
    
    # Create output directory
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("ðŸ“Š Preparing calibration dataset...")
    
    # For Whisper, we need audio mel-spectrogram inputs
    # Create synthetic calibration data (in production, use real audio samples)
    input_shape = model.input(0).shape
    
    # Generate calibration dataset
    calibration_data = []
    for i in range(calibration_samples):
        # Create mel-spectrogram-like input
        # Shape is typically [1, 80, 3000] for Whisper
        if len(input_shape) == 3:
            batch_size = 1
            n_mels = 80
            n_frames = 3000
            sample = np.random.randn(batch_size, n_mels, n_frames).astype(np.float32)
        else:
            # Fallback for different input shapes
            sample = np.random.randn(*input_shape).astype(np.float32)
        
        calibration_data.append(sample)
    
    logger.info(f"ðŸ“ˆ Generated {len(calibration_data)} calibration samples")
    
    # Configure INT8 quantization
    logger.info("ðŸ”§ Configuring INT8 quantization for Intel iGPU...")
    
    # Quantization configuration optimized for Intel iGPU
    quantization_config = {
        "preset": "mixed",  # Mixed precision (INT8 + FP16) for best accuracy
        "target_device": "GPU",  # Intel integrated GPU
        "stat_subset_size": calibration_samples,
        "stat_batch_size": 1,
        "algorithms": [
            {
                "name": "DefaultQuantization",
                "params": {
                    "preset": "performance",  # Optimize for speed
                    "stat_subset_size": calibration_samples,
                    "target_device": "GPU",
                    "model_type": "transformer",  # Whisper is transformer-based
                    "inplace_statistics": True,
                    "ignored_scopes": []  # Can add layer names to skip quantization
                }
            }
        ]
    }
    
    # Apply INT8 quantization
    logger.info("âš¡ Applying INT8 quantization...")
    
    # Use NNCF for INT8 quantization
    logger.info("Using NNCF weight compression to INT8...")
    
    # Direct weight compression to INT8
    # This provides excellent performance on Intel iGPU
    compressed_model = nncf.compress_weights(
        model,
        mode=nncf.CompressWeightsMode.INT8,  # INT8 mode
        ratio=0.9,  # Compress 90% of weights
        group_size=128,  # Group size for quantization
        all_layers=True  # Quantize all eligible layers
    )
    quantized_model = compressed_model
    
    # Save quantized model
    logger.info(f"ðŸ’¾ Saving INT8 model to {output_path}")
    ov.save_model(quantized_model, output_path)
    
    # Save binary weights
    bin_path = str(output_path).replace('.xml', '.bin')
    logger.info(f"âœ… INT8 model saved: {output_path}")
    
    # Calculate compression ratio
    original_size = Path(model_xml).stat().st_size + Path(str(model_xml).replace('.xml', '.bin')).stat().st_size
    quantized_size = Path(output_path).stat().st_size + Path(bin_path).stat().st_size
    compression_ratio = original_size / quantized_size
    
    logger.info(f"ðŸ“‰ Compression ratio: {compression_ratio:.2f}x")
    logger.info(f"ðŸ“¦ Original size: {original_size / 1024 / 1024:.2f} MB")
    logger.info(f"ðŸ“¦ Quantized size: {quantized_size / 1024 / 1024:.2f} MB")
    
    return output_path

def quantize_whisper_model(model_dir: str, output_dir: str):
    """Quantize all OpenVINO model components to INT8"""
    
    model_path = Path(model_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Copy all non-model files
    for file in model_path.glob("*"):
        if file.suffix not in ['.xml', '.bin']:
            shutil.copy2(file, output_path / file.name)
    
    # Quantize encoder
    if (model_path / "openvino_encoder_model.xml").exists():
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ¯ Quantizing ENCODER to INT8...")
        logger.info("="*60)
        quantize_openvino_model_int8(
            str(model_path / "openvino_encoder_model.xml"),
            str(output_path / "openvino_encoder_model.xml"),
            calibration_samples=50
        )
    
    # Quantize decoder
    if (model_path / "openvino_decoder_model.xml").exists():
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ¯ Quantizing DECODER to INT8...")
        logger.info("="*60)
        quantize_openvino_model_int8(
            str(model_path / "openvino_decoder_model.xml"),
            str(output_path / "openvino_decoder_model.xml"),
            calibration_samples=50
        )
    
    # Quantize decoder with past (if exists)
    if (model_path / "openvino_decoder_with_past_model.xml").exists():
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ¯ Quantizing DECODER WITH PAST to INT8...")
        logger.info("="*60)
        quantize_openvino_model_int8(
            str(model_path / "openvino_decoder_with_past_model.xml"),
            str(output_path / "openvino_decoder_with_past_model.xml"),
            calibration_samples=30
        )
    
    # Create optimized config for INT8
    config = {
        "model_type": "whisper",
        "quantization": "int8",
        "optimized_for": "intel_igpu",
        "performance_hints": {
            "PERFORMANCE_HINT": "LATENCY",
            "INFERENCE_PRECISION_HINT": "i8",  # INT8 precision
            "GPU_THROUGHPUT_STREAMS": "1",
            "CACHE_DIR": "/app/cache",
            "ENFORCE_BF16": "NO",  # Disable BF16, use INT8
            "OPTIMIZE_PREPROCESSOR": "GPU"
        }
    }
    
    with open(output_path / "quantization_config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    logger.info(f"\nâœ… Model quantized to INT8: {output_path}")
    
    # Performance expectations
    logger.info("\n" + "="*60)
    logger.info("ðŸš€ Expected INT8 Performance on Intel iGPU:")
    logger.info("="*60)
    logger.info("â€¢ 2-4x faster inference vs FP16")
    logger.info("â€¢ 4x smaller model size")
    logger.info("â€¢ 50-70% lower memory bandwidth")
    logger.info("â€¢ Better EU (Execution Unit) utilization")
    logger.info("â€¢ Optimized for Intel Xe/Arc architecture")
    logger.info("="*60)

def main():
    parser = argparse.ArgumentParser(description="Quantize OpenVINO Whisper models to INT8")
    parser.add_argument("--model-dir", type=str, required=True, help="Path to OpenVINO model directory")
    parser.add_argument("--output-dir", type=str, required=True, help="Output directory for INT8 model")
    
    args = parser.parse_args()
    
    logger.info("ðŸ¦„ Unicorn Amanuensis - INT8 Quantization for Intel iGPU")
    logger.info("="*60)
    
    quantize_whisper_model(args.model_dir, args.output_dir)

if __name__ == "__main__":
    # Example usage:
    # python quantize_to_int8.py --model-dir ~/openvino-models/whisper-base-openvino --output-dir ~/openvino-models/whisper-base-int8
    
    if len(sys.argv) == 1:
        # Run default quantization for base model
        logger.info("Running default quantization for whisper-base...")
        quantize_whisper_model(
            "/home/ucadmin/openvino-models/whisper-base-openvino",
            "/home/ucadmin/openvino-models/whisper-base-int8"
        )
    else:
        main()