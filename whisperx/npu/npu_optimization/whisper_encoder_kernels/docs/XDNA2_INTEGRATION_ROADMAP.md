# XDNA2 Integration Roadmap

**Date**: November 17, 2025
**Version**: 1.0
**Target**: Prepare XDNA1 codebase for seamless XDNA2 migration
**Goal**: 95% code reuse, 1.8-2x performance scaling

---

## Executive Summary

This roadmap outlines a **4-phase approach** to prepare the Whisper encoder NPU kernels for XDNA2 (Strix) hardware while optimizing for current XDNA1 (Phoenix) deployment.

### Timeline Overview

| Phase | Focus | Duration | Completion Date | Status |
|-------|-------|----------|-----------------|--------|
| **Phase 1** | Copy Optimized Kernels | 1 week | Today | ðŸ”„ In Progress |
| **Phase 2** | IRON API Migration | 1 week | Week of Nov 24 | ðŸ“… Scheduled |
| **Phase 3** | Multi-Column Optimization | 1 week | Week of Dec 1 | ðŸ“… Planned |
| **Phase 4** | XDNA2 Preparation | 2 weeks | Week of Dec 8 | ðŸ”œ Future |

**Total Timeline**: 5 weeks to XDNA2-ready codebase

### Key Milestones

- âœ… **Week 1**: All XDNA1 kernels working and optimized
- ðŸŽ¯ **Week 2**: IRON API migration complete
- ðŸŽ¯ **Week 3**: 4-column parallel execution optimized
- ðŸŽ¯ **Week 5**: Ready for XDNA2 hardware (8-column scaling)

---

## Table of Contents

1. [Phase 1: Copy Optimized Kernels](#phase-1-copy-optimized-kernels)
2. [Phase 2: IRON API Migration](#phase-2-iron-api-migration)
3. [Phase 3: Multi-Column Optimization](#phase-3-multi-column-optimization)
4. [Phase 4: XDNA2 Preparation](#phase-4-xdna2-preparation)
5. [Success Criteria](#success-criteria)
6. [Risk Mitigation](#risk-mitigation)

---

## Phase 1: Copy Optimized Kernels

**Duration**: 1 week (Nov 17-24, 2025)
**Status**: ðŸ”„ In Progress (Day 3)
**Objective**: Consolidate working XDNA1 kernels into organized structure

### Goals

1. âœ… Organize existing kernels into common/xdna1 structure
2. âœ… Copy all working C++ kernel implementations
3. âœ… Copy all working MLIR definitions
4. âœ… Create baseline XDNA1 directory structure
5. ðŸ”„ Update import paths and build scripts
6. ðŸ”„ Verify all tests pass with new structure

### Tasks Breakdown

#### Day 1: Directory Structure Setup âœ…

**Actions Completed**:
```bash
# Created directory structure
mkdir -p kernels/common
mkdir -p kernels/xdna1
mkdir -p runtime/common
mkdir -p runtime/xdna1
mkdir -p build/xdna1
mkdir -p tests/common
mkdir -p tests/xdna1
```

**Result**: Clean separation of shared vs platform-specific code

#### Day 2-3: Kernel Migration ðŸ”„

**C++ Kernels** (move to `kernels/common/`):
- [x] `attention_int8_64x64.c` â†’ `kernels/common/attention_int8.c`
- [x] `matmul_int8_32x32.c` â†’ `kernels/common/matmul_int8.c`
- [x] `gelu_int8.c` â†’ `kernels/common/gelu_int8.c`
- [x] `layernorm_int8.c` â†’ `kernels/common/layernorm_int8.c`
- [ ] Create `kernels/common/kernel_common.h` (shared definitions)
- [ ] Create `kernels/common/README.md` (documentation)

**MLIR Kernels** (move to `kernels/xdna1/`):
- [ ] `attention_64x64.mlir` â†’ `kernels/xdna1/attention_xdna1.mlir`
- [ ] `matmul_32x32.mlir` â†’ `kernels/xdna1/matmul_xdna1.mlir`
- [ ] `gelu_simple.mlir` â†’ `kernels/xdna1/gelu_xdna1.mlir`
- [ ] `layernorm_simple.mlir` â†’ `kernels/xdna1/layernorm_xdna1.mlir`

**Runtime Wrappers** (move to `runtime/xdna1/`):
- [ ] `npu_attention_wrapper.py` â†’ `runtime/xdna1/npu_attention_wrapper.py`
- [ ] `npu_matmul_wrapper_batched.py` â†’ `runtime/xdna1/npu_matmul_wrapper.py`

#### Day 4: Build System Update

**Update Compilation Scripts**:
```bash
# kernels/xdna1/compile_xdna1.sh
#!/bin/bash

# Compile all XDNA1 kernels
KERNEL_DIR="$(dirname $0)"
COMMON_DIR="$KERNEL_DIR/../common"
BUILD_DIR="../../build/xdna1"

# Compile attention kernel
aie-opt \
  --aie-canonicalize-device \
  --aie-objectFifo-stateful-transform \
  --aie-create-pathfinder-flows \
  $KERNEL_DIR/attention_xdna1.mlir | \
aie-translate --aie-generate-xclbin \
  --peano-install-dir=/opt/peano \
  -o $BUILD_DIR/attention_64x64.xclbin

# Compile matmul kernel
aie-opt \
  --aie-canonicalize-device \
  --aie-objectFifo-stateful-transform \
  $KERNEL_DIR/matmul_xdna1.mlir | \
aie-translate --aie-generate-xclbin \
  -o $BUILD_DIR/matmul_32x32.xclbin

# Compile GELU kernel
aie-opt \
  --aie-canonicalize-device \
  $KERNEL_DIR/gelu_xdna1.mlir | \
aie-translate --aie-generate-xclbin \
  -o $BUILD_DIR/gelu_2048.xclbin

# Compile LayerNorm kernel
aie-opt \
  --aie-canonicalize-device \
  $KERNEL_DIR/layernorm_xdna1.mlir | \
aie-translate --aie-generate-xclbin \
  -o $BUILD_DIR/layernorm_512.xclbin

echo "âœ… All XDNA1 kernels compiled successfully"
```

#### Day 5: Testing and Validation

**Test Plan**:
1. Run existing test suite from new locations
2. Verify all XCLBINs load and execute
3. Validate accuracy against reference
4. Benchmark performance (should match current)

**Expected Results**:
- All tests pass with identical results
- No performance regression
- Clean separation achieved

### Deliverables

- [x] Clean directory structure
- [ ] All C++ kernels in `kernels/common/`
- [ ] All MLIR kernels in `kernels/xdna1/`
- [ ] All runtime wrappers in `runtime/xdna1/`
- [ ] Updated build scripts
- [ ] Passing test suite
- [ ] Documentation: `PHASE1_COMPLETE.md`

### Success Metrics

| Metric | Target | Current |
|--------|--------|---------|
| **Tests Passing** | 100% | 89% (pending migration) |
| **Performance** | No regression | TBD |
| **Accuracy** | No degradation | TBD |
| **Build Time** | <2 seconds | 0.5-1.0 sec (current) |

---

## Phase 2: IRON API Migration

**Duration**: 1 week (Nov 24-Dec 1, 2025)
**Status**: ðŸ“… Scheduled
**Objective**: Migrate from manual DMA to IRON ObjectFIFO API

### Goals

1. Replace manual `aie.dma_start` with `aie.objectFifo.createObjectFifo`
2. Simplify buffer management and data movement
3. Improve portability (IRON works on XDNA1 and XDNA2)
4. Enable easier multi-column scaling
5. Maintain or improve performance

### Background: Why IRON?

**Current Approach** (Manual DMA):
```mlir
// Manual DMA configuration (complex, error-prone)
%buf_in = aie.buffer(%tile) : memref<256xi8>
%buf_out = aie.buffer(%tile) : memref<256xi8>

%dma = aie.dma_start(S2MM, 0, ^bd0, ^end)
^bd0:
  aie.use_lock(%lock_in, AcquireGreaterEqual, 1)
  aie.dma_bd(%buf_in : memref<256xi8>, 0, 256)
  aie.use_lock(%lock_in, Release, 0)
  aie.next_bd ^end
^end:
  aie.end
```

**IRON Approach** (ObjectFIFO):
```mlir
// IRON ObjectFIFO (simple, portable)
%fifo_in = aie.objectFifo.createObjectFifo(%tile_shim, {%tile_compute},
                                            256 : i32) : !aie.objectFifo<memref<256xi8>>

// Data movement is automatic
aie.objectFifo.link [ %fifo_in ] -> [ %fifo_compute ]
```

**Benefits**:
- 50% less code
- Automatic lock management
- Better column scaling
- XDNA2-ready

### Tasks Breakdown

#### Day 1-2: Attention Kernel Migration

**Current MLIR** (`kernels/xdna1/attention_xdna1.mlir`):
- Manual DMA setup for Q, K, V inputs
- Manual DMA for output
- Lock management code
- Tile coordination

**New MLIR with IRON**:
```mlir
module @attention_xdna1_iron {
  aie.device(npu1) {
    // Define ObjectFIFOs for inputs
    %fifo_Q = aie.objectFifo.createObjectFifo(%tile_0_0, {%tile_0_2},
                                               4096 : i32) : !aie.objectFifo<memref<64x64xi8>>
    %fifo_K = aie.objectFifo.createObjectFifo(%tile_1_0, {%tile_1_2},
                                               4096 : i32) : !aie.objectFifo<memref<64x64xi8>>
    %fifo_V = aie.objectFifo.createObjectFifo(%tile_2_0, {%tile_2_2},
                                               4096 : i32) : !aie.objectFifo<memref<64x64xi8>>

    // Output FIFO
    %fifo_out = aie.objectFifo.createObjectFifo(%tile_3_2, {%tile_3_0},
                                                 4096 : i32) : !aie.objectFifo<memref<64x64xi8>>

    // Compute core - data movement is automatic
    aie.core(%tile_3_2) {
      %Q = aie.objectFifo.acquire %fifo_Q : !aie.objectFifoSubview<memref<64x64xi8>>
      %K = aie.objectFifo.acquire %fifo_K : !aie.objectFifoSubview<memref<64x64xi8>>
      %V = aie.objectFifo.acquire %fifo_V : !aie.objectFifoSubview<memref<64x64xi8>>
      %out = aie.objectFifo.acquire %fifo_out : !aie.objectFifoSubview<memref<64x64xi8>>

      // Call C++ kernel (unchanged)
      func.call @attention_int8(%Q, %K, %V, %out) : (memref<64x64xi8>, memref<64x64xi8>,
                                                      memref<64x64xi8>, memref<64x64xi8>) -> ()

      aie.objectFifo.release %fifo_Q
      aie.objectFifo.release %fifo_K
      aie.objectFifo.release %fifo_V
      aie.objectFifo.release %fifo_out

      aie.end
    }
  }
}
```

**Testing**: Verify attention kernel works with IRON API

#### Day 3-4: MatMul and GELU Migration

**Similar migration for**:
- MatMul kernel (32x32 tiles)
- GELU kernel (2048 elements)
- LayerNorm kernel (512 elements)

**Pattern**:
1. Replace `aie.buffer` + `aie.dma_start` with `aie.objectFifo.createObjectFifo`
2. Replace locks with ObjectFIFO acquire/release
3. Simplify tile coordination
4. Test and validate

#### Day 5: Performance Validation

**Benchmark Suite**:
```python
# tests/common/benchmark_iron_vs_manual.py
def benchmark_attention_iron():
    # Test with IRON API
    npu = NPUXDNA1()
    npu.load_xclbin("build/xdna1/attention_iron.xclbin")

    times = []
    for _ in range(100):
        start = time.perf_counter()
        npu.attention(Q, K, V)
        times.append(time.perf_counter() - start)

    return np.mean(times), np.std(times)

def compare_apis():
    manual_mean, manual_std = benchmark_attention_manual()
    iron_mean, iron_std = benchmark_attention_iron()

    print(f"Manual DMA: {manual_mean*1000:.2f} Â± {manual_std*1000:.2f} ms")
    print(f"IRON API:   {iron_mean*1000:.2f} Â± {iron_std*1000:.2f} ms")
    print(f"Speedup:    {manual_mean/iron_mean:.2f}x")
```

**Expected Results**:
- IRON should be 0.9-1.1x manual DMA (similar performance)
- Simpler code (50% less MLIR)
- Easier to maintain

### Deliverables

- [ ] All kernels migrated to IRON API
- [ ] Performance comparison report
- [ ] Updated build scripts
- [ ] Documentation: `IRON_MIGRATION_COMPLETE.md`

### Success Metrics

| Metric | Target | Baseline (Manual DMA) |
|--------|--------|----------------------|
| **Attention Latency** | <4 ms | 3.6 ms |
| **MatMul Latency** | <0.5 ms | 0.46 ms |
| **MLIR LOC** | <50% of manual | 600 LOC (manual) |
| **Portability** | XDNA1 + XDNA2 | XDNA1 only |

---

## Phase 3: Multi-Column Optimization

**Duration**: 1 week (Dec 1-8, 2025)
**Status**: ðŸ“… Planned
**Objective**: Optimize parallel execution across 4 XDNA1 columns

### Goals

1. Utilize all 4 XDNA1 columns simultaneously
2. Minimize column idle time
3. Implement work distribution strategies
4. Prepare column-scaling patterns for XDNA2 (8 columns)
5. Achieve near-linear 4-way speedup where applicable

### Multi-Column Strategies

#### Strategy 1: Spatial Parallelism (MatMul)

**Concept**: Split matrix into 4 tiles, compute on 4 columns in parallel

**Example**: 64Ã—64 MatMul
```
Original: 64Ã—64 on 1 column â†’ 0.46 ms

Optimized: Split into 4x 32Ã—32 tiles on 4 columns
Column 0: Tile (0,0)   â”
Column 1: Tile (0,1)   â”œâ”€ Parallel execution
Column 2: Tile (1,0)   â”‚
Column 3: Tile (1,1)   â”˜

Result: 64Ã—64 in ~0.15 ms (3x speedup)
```

**MLIR Implementation**:
```mlir
module @matmul_4col {
  aie.device(npu1) {
    // 4 columns, each processes 1/4 of matrix
    %tile_0_2 = aie.tile(0, 2)  // Tile (0,0)
    %tile_1_2 = aie.tile(1, 2)  // Tile (0,1)
    %tile_2_2 = aie.tile(2, 2)  // Tile (1,0)
    %tile_3_2 = aie.tile(3, 2)  // Tile (1,1)

    // ObjectFIFOs for data distribution
    %fifo_A = aie.objectFifo.createObjectFifo(%tile_0_0,
              {%tile_0_2, %tile_1_2, %tile_2_2, %tile_3_2}, ...)

    // Each column processes independently
    aie.core(%tile_0_2) { /* matmul tile (0,0) */ }
    aie.core(%tile_1_2) { /* matmul tile (0,1) */ }
    aie.core(%tile_2_2) { /* matmul tile (1,0) */ }
    aie.core(%tile_3_2) { /* matmul tile (1,1) */ }
  }
}
```

#### Strategy 2: Temporal Parallelism (Attention)

**Concept**: Pipeline attention heads across 4 columns

**Example**: 8-head attention
```
Column 0: Heads 0, 4   â”
Column 1: Heads 1, 5   â”œâ”€ Process 2 heads per column
Column 2: Heads 2, 6   â”‚
Column 3: Heads 3, 7   â”˜

Result: 8 heads in ~2Ã— single-head time (4x speedup)
```

#### Strategy 3: Hybrid Parallelism (Full Encoder)

**Concept**: Overlap different operations across columns

**Example**: Encoder layer
```
Time 0:
  Column 0: Attention head 0
  Column 1: Attention head 1
  Column 2: Attention head 2
  Column 3: Attention head 3

Time 1:
  Column 0: FFN matmul 1 (frame 0)
  Column 1: FFN matmul 1 (frame 1)
  Column 2: Attention head 4
  Column 3: Attention head 5

Overlapped execution â†’ Higher NPU utilization
```

### Tasks Breakdown

#### Day 1-2: MatMul 4-Column Implementation

- [ ] Implement tile splitting logic
- [ ] Create 4-column MLIR
- [ ] Test with 64Ã—64, 128Ã—128 matrices
- [ ] Benchmark vs single-column

**Target**: 3-3.5x speedup (accounting for overhead)

#### Day 3-4: Attention 4-Column Implementation

- [ ] Implement head distribution
- [ ] Pipeline 8 heads across 4 columns
- [ ] Test with Whisper dimensions (1500 frames)
- [ ] Benchmark vs single-column

**Target**: 3-3.5x speedup for multi-head attention

#### Day 5: Integration and Optimization

- [ ] Combine MatMul + Attention in single encoder layer
- [ ] Optimize column utilization
- [ ] Minimize synchronization overhead
- [ ] End-to-end encoder layer benchmark

**Target**: 2.5-3x speedup for full encoder layer

### Deliverables

- [ ] 4-column MatMul kernel
- [ ] 4-column Attention kernel
- [ ] Integration example (encoder layer)
- [ ] Performance report
- [ ] Documentation: `MULTI_COLUMN_OPTIMIZATION.md`

### Success Metrics

| Operation | 1-Column | 4-Column | Speedup | Target |
|-----------|----------|----------|---------|--------|
| **MatMul 64Ã—64** | 0.46 ms | 0.15 ms | 3.1x | 3-3.5x |
| **Attention (8 heads)** | 28 ms | 9 ms | 3.1x | 3-3.5x |
| **Encoder Layer** | 45 ms | 15 ms | 3.0x | 2.5-3x |

---

## Phase 4: XDNA2 Preparation

**Duration**: 2 weeks (Dec 8-22, 2025)
**Status**: ðŸ”œ Future
**Objective**: Create XDNA2 variants and validate scaling

### Goals

1. Create `kernels/xdna2/` with 8-column MLIR variants
2. Implement `runtime/xdna2/` with 8-column mapping
3. Update platform detection
4. Document XDNA2-specific optimizations
5. Prepare for hardware availability (testing deferred)

### Tasks Breakdown

#### Week 1: XDNA2 Kernel Variants

**Day 1-2: MLIR Conversion**

**Process**:
1. Copy from `kernels/xdna1/` to `kernels/xdna2/`
2. Update device target: `aie.device(npu1)` â†’ `aie.device(npu2)`
3. Update tile coordinates: 4 columns â†’ 8 columns
4. Adjust parallelism: 4-way â†’ 8-way

**Example Diff**:
```diff
# kernels/xdna1/matmul_xdna1.mlir â†’ kernels/xdna2/matmul_xdna2.mlir

module @matmul_xdna1 {
-  aie.device(npu1) {
+  aie.device(npu2) {
     // Column mapping
-    %tile_0_2 = aie.tile(0, 2)
-    %tile_1_2 = aie.tile(1, 2)
-    %tile_2_2 = aie.tile(2, 2)
-    %tile_3_2 = aie.tile(3, 2)
+    %tile_0_2 = aie.tile(0, 2)
+    %tile_1_2 = aie.tile(1, 2)
+    %tile_2_2 = aie.tile(2, 2)
+    %tile_3_2 = aie.tile(3, 2)
+    %tile_4_2 = aie.tile(4, 2)  // NEW
+    %tile_5_2 = aie.tile(5, 2)  // NEW
+    %tile_6_2 = aie.tile(6, 2)  // NEW
+    %tile_7_2 = aie.tile(7, 2)  // NEW
   }
}
```

**C++ Kernels**: No changes (already in `kernels/common/`)

**Day 3-4: Runtime Wrapper**

**Create `runtime/xdna2/npu_xdna2.py`**:
```python
from runtime.common.npu_base import NPUBase

class NPUXDNA2(NPUBase):
    """XDNA2 (Strix) NPU Runtime"""

    NUM_COLUMNS = 8
    DEVICE_NAME = "npu2"

    def __init__(self):
        super().__init__()
        self.xclbin_dir = "build/xdna2"

    def load_kernels(self):
        """Load XDNA2 XCLBINs"""
        self.load_xclbin(f"{self.xclbin_dir}/attention_128x64.xclbin")
        self.load_xclbin(f"{self.xclbin_dir}/matmul_64x64.xclbin")
        # Larger tile sizes optimized for 8 columns

    def get_tile_mapping(self, operation_width):
        """Map operation to 8 columns"""
        if operation_width <= 8:
            return list(range(operation_width))
        else:
            # Time-multiplex on 8 columns
            chunks = (operation_width + 7) // 8
            return [i % 8 for i in range(operation_width)]
```

**Day 5: Platform Detection**

**Update `runtime/__init__.py`**:
```python
import xrt

def create_npu():
    """Factory: auto-detect XDNA1 or XDNA2"""
    device = xrt.xrt_device(0)
    device_name = device.get_info(xrt.info_device.name)

    if "Phoenix" in device_name or "XDNA1" in device_name:
        from runtime.xdna1.npu_xdna1 import NPUXDNA1
        return NPUXDNA1()
    elif "Strix" in device_name or "XDNA2" in device_name:
        from runtime.xdna2.npu_xdna2 import NPUXDNA2
        return NPUXDNA2()
    else:
        # Fallback: Try querying column count
        # (future-proof for XDNA3+)
        num_columns = detect_column_count(device)
        if num_columns == 4:
            return NPUXDNA1()
        elif num_columns == 8:
            return NPUXDNA2()
        else:
            raise RuntimeError(f"Unknown NPU: {device_name}")
```

#### Week 2: Documentation and Validation Framework

**Day 6-7: Comprehensive Documentation**

- [ ] `docs/XDNA2_MIGRATION_GUIDE.md`
- [ ] `docs/XDNA2_PERFORMANCE_TUNING.md`
- [ ] `docs/XDNA2_TESTING_PLAN.md`

**Day 8-10: Validation Framework (No Hardware)**

**Create test infrastructure for XDNA2**:
```python
# tests/xdna2/test_xdna2_kernels.py
import pytest

@pytest.mark.xdna2
@pytest.mark.skipif(not has_xdna2_hardware(), reason="XDNA2 hardware not available")
def test_xdna2_attention():
    """Test attention on XDNA2 - runs only when hardware available"""
    npu = NPUXDNA2()

    Q, K, V = generate_test_inputs(128, 64)
    output = npu.attention(Q, K, V)

    # Validate accuracy
    assert validate_accuracy(output) > 0.95

    # Validate performance (should be ~1.8-2x XDNA1)
    latency = benchmark(npu.attention, Q, K, V)
    assert latency < 2.0, f"XDNA2 attention too slow: {latency}ms"
```

**Mock Testing** (without hardware):
```python
# tests/xdna2/test_xdna2_mock.py
def test_xdna2_column_mapping():
    """Test 8-column mapping logic (no hardware needed)"""
    npu = NPUXDNA2()

    # Test various operation widths
    assert npu.get_tile_mapping(4) == [0, 1, 2, 3]
    assert npu.get_tile_mapping(8) == [0, 1, 2, 3, 4, 5, 6, 7]
    assert len(npu.get_tile_mapping(16)) == 16  # Time-multiplexed
```

### Deliverables

- [ ] `kernels/xdna2/` with all 8-column MLIR variants
- [ ] `runtime/xdna2/` with 8-column runtime
- [ ] Platform auto-detection working
- [ ] Test framework ready for XDNA2 hardware
- [ ] Comprehensive documentation
- [ ] Documentation: `XDNA2_READY_REPORT.md`

### Success Metrics (Projected)

| Metric | XDNA1 | XDNA2 Target | Scaling |
|--------|-------|--------------|---------|
| **MatMul 64Ã—64** | 0.15 ms (4-col) | 0.08 ms | 1.88x |
| **Attention (8 heads)** | 9 ms (4-col) | 5 ms | 1.80x |
| **Encoder Layer** | 15 ms (4-col) | 8.5 ms | 1.76x |
| **Full Encoder (6 layers)** | 90 ms | 50 ms | 1.80x |

**Note**: Targets based on 1.8-2x theoretical scaling from 4â†’8 columns

---

## Success Criteria

### Phase 1 Success âœ…

- [ ] All kernels in organized structure
- [ ] All tests passing with new paths
- [ ] No performance regression
- [ ] Documentation complete

### Phase 2 Success ðŸŽ¯

- [ ] All kernels using IRON API
- [ ] 0.9-1.1x performance vs manual DMA
- [ ] 50% reduction in MLIR code
- [ ] XDNA2-compatible MLIR

### Phase 3 Success ðŸŽ¯

- [ ] 3-3.5x speedup on 4-column operations
- [ ] MatMul, Attention, GELU all multi-column
- [ ] Full encoder layer 2.5-3x faster
- [ ] Column utilization >80%

### Phase 4 Success ðŸŽ¯

- [ ] XDNA2 kernels created (8 columns)
- [ ] Runtime auto-detection working
- [ ] Test framework ready
- [ ] Documentation complete
- [ ] Code ready for XDNA2 hardware day 1

### Overall Success Metrics

**Code Reuse**:
- Target: 95% shared between XDNA1/XDNA2
- Current: 100% C++ kernels shared âœ…
- Target: MLIR templates reusable with param changes

**Performance**:
- XDNA1: 2.5-3x speedup from multi-column optimization
- XDNA2: Additional 1.8-2x from 8 columns
- Combined: 4.5-6x faster than single-column baseline

**Timeline**:
- 5 weeks from start to XDNA2-ready
- Weekly milestones and deliverables
- Continuous validation and testing

---

## Risk Mitigation

### Risk 1: IRON API Performance Regression

**Probability**: Medium
**Impact**: Medium

**Mitigation**:
- Benchmark before/after IRON migration
- Keep manual DMA versions as fallback
- Profile and optimize IRON patterns

**Contingency**: Stay with manual DMA for critical kernels if needed

### Risk 2: Multi-Column Synchronization Overhead

**Probability**: Medium
**Impact**: Medium

**Mitigation**:
- Start with simple 2-column tests
- Measure synchronization cost
- Use async patterns where possible

**Contingency**: Time-multiplex if synchronization too costly

### Risk 3: XDNA2 Hardware Delayed

**Probability**: Low
**Impact**: Low

**Mitigation**:
- Phase 4 prepares code but doesn't require hardware
- Mock testing validates logic
- Ready to test when hardware arrives

**Contingency**: Continue optimizing XDNA1 in parallel

### Risk 4: Unexpected XDNA2 Architectural Differences

**Probability**: Low
**Impact**: High

**Mitigation**:
- Maintain close communication with AMD
- Review XDNA2 specifications early
- Design for flexibility

**Contingency**: Adapt MLIR templates as needed (still 95% code reuse)

---

## Quick Reference Timeline

```
Nov 17 â”œâ”€ Phase 1 Start: Organize kernels
Nov 24 â”œâ”€ Phase 1 Complete: Clean structure âœ…
       â”‚
       â”œâ”€ Phase 2 Start: IRON API migration
Dec 1  â”œâ”€ Phase 2 Complete: Modern API âœ…
       â”‚
       â”œâ”€ Phase 3 Start: Multi-column optimization
Dec 8  â”œâ”€ Phase 3 Complete: 4-column parallel âœ…
       â”‚
       â”œâ”€ Phase 4 Start: XDNA2 preparation
Dec 22 â”œâ”€ Phase 4 Complete: XDNA2-ready âœ…
       â”‚
Future â”œâ”€ XDNA2 Hardware Available
       â””â”€ Test on XDNA2, validate 1.8-2x scaling
```

---

## Summary

### What We're Building

**Immediate** (Phases 1-3, 3 weeks):
- Clean, organized codebase
- Modern IRON API
- 4-column parallel execution
- 2.5-3x speedup on XDNA1

**Future-Proof** (Phase 4, 2 weeks):
- XDNA2 variants ready
- 8-column scaling prepared
- Auto-detection working
- Additional 1.8-2x on XDNA2

**Total Impact**:
- **4.5-6x speedup** when XDNA2 available
- **95% code reuse** between generations
- **Maintainable** architecture
- **Production-ready** incrementally

---

**Document Version**: 1.0
**Last Updated**: November 17, 2025
**Next Review**: After Phase 1 completion (Nov 24)
**Maintained By**: NPU Documentation Team
