/**
 * INT8 Attention Mechanism for Whisper Encoder - SCALED TO 64x64
 * Optimized for AIE2 vector operations
 *
 * Implements: Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V
 *
 * For Whisper base:
 *   Sequence length: 1500 (30 seconds @ 16kHz with 10ms hop)
 *   Hidden dim: 512
 *   Heads: 8
 *   Head dim: 64 (512 / 8)
 *
 * This kernel processes ONE attention head on 64x64 tiles
 * SCALED FROM 16x16 FOR PRODUCTION WHISPER WORKLOADS
 */

#include <stdint.h>
#include <string.h>

/**
 * Fast integer square root (for scaling factor sqrt(d_k))
 * Used for attention scaling: 1/sqrt(64) ≈ 1/8
 */
static inline uint16_t isqrt(uint32_t n) {
    if (n == 0) return 0;
    uint32_t x = n;
    uint32_t y = (x + 1) / 2;
    while (y < x) {
        x = y;
        y = (x + n / x) / 2;
    }
    return (uint16_t)x;
}

/**
 * Softmax approximation for INT8 - SCALED TO 64 ELEMENTS
 * Uses lookup table for exp() and integer arithmetic
 *
 * Input: [N] int8 values (attention scores)
 * Output: [N] int8 values (probabilities summing to ~127)
 */
void softmax_int8_64(const int8_t* input, int8_t* output, uint32_t N) {
    // Find max value for numerical stability
    int8_t max_val = input[0];
    for (uint32_t i = 1; i < N; i++) {
        if (input[i] > max_val) max_val = input[i];
    }

    // Compute exp(x - max) using approximation
    // For INT8, we use: exp(x) ≈ 1 + x + x²/2 (Taylor series, 2 terms)
    int32_t sum = 0;
    int32_t exp_vals[64];  // Temporary storage (max 64 elements)

    for (uint32_t i = 0; i < N; i++) {
        int32_t x = input[i] - max_val;  // Shift for stability

        // Clamp to prevent overflow
        if (x < -64) x = -64;
        if (x > 0) x = 0;  // exp(x - max) where x <= max, so always <= 0

        // Approximate exp: 64 * (1 + x/64 + x²/8192)
        // For x in [-64, 0], this gives values in [~0, 64]
        int32_t exp_val = 64 + x + (x * x) / 128;
        if (exp_val < 0) exp_val = 0;

        exp_vals[i] = exp_val;
        sum += exp_val;
    }

    // Normalize: output[i] = exp_vals[i] * 127 / sum
    for (uint32_t i = 0; i < N; i++) {
        if (sum > 0) {
            int32_t normalized = (exp_vals[i] * 127) / sum;
            if (normalized > 127) normalized = 127;
            output[i] = (int8_t)normalized;
        } else {
            output[i] = 0;
        }
    }
}

/**
 * Scaled dot-product attention for 64x64 tiles - COMBINED QKV VERSION
 *
 * Due to Phoenix NPU DMA channel limits (2 per ShimNOC tile),
 * we take a single combined buffer containing Q, K, V
 *
 * QKV_combined layout:
 *   Bytes 0-4095: Q matrix [64 x 64]
 *   Bytes 4096-8191: K matrix [64 x 64]
 *   Bytes 8192-12287: V matrix [64 x 64]
 *
 * MEMORY USAGE: 64x64 int32 accumulator = 16KB (fits in AIE2 32KB memory)
 */
void attention_64x64(
    const int8_t* QKV_combined,  // [12288] combined Q+K+V buffer
    int8_t* output,              // [64 x 64] output matrix
    uint32_t scale_shift         // Right shift for Q@K^T (divide by sqrt(d_k))
) {
    // Unpack Q, K, V from combined buffer
    const int8_t* Q = &QKV_combined[0];       // Bytes 0-4095
    const int8_t* K = &QKV_combined[4096];    // Bytes 4096-8191
    const int8_t* V = &QKV_combined[8192];    // Bytes 8192-12287

    // Step 1: Compute attention scores: Q @ K^T
    // Result: [64 x 64] scores
    int8_t scores[4096];  // 64x64 = 4096 elements

    for (uint32_t i = 0; i < 64; i++) {
        for (uint32_t j = 0; j < 64; j++) {
            int32_t score = 0;

            // Dot product: Q[i, :] @ K[j, :]
            for (uint32_t k = 0; k < 64; k++) {
                score += (int32_t)Q[i * 64 + k] * (int32_t)K[j * 64 + k];
            }

            // Scale by 1/sqrt(d_k)
            score >>= scale_shift;

            // Clamp to INT8 range
            if (score > 127) score = 127;
            if (score < -128) score = -128;

            scores[i * 64 + j] = (int8_t)score;
        }
    }

    // Step 2: Apply softmax row-wise (each query attends to all keys)
    int8_t attention_weights[4096];
    for (uint32_t i = 0; i < 64; i++) {
        softmax_int8_64(&scores[i * 64], &attention_weights[i * 64], 64);
    }

    // Step 3: Weighted sum: attention_weights @ V
    for (uint32_t i = 0; i < 64; i++) {
        for (uint32_t j = 0; j < 64; j++) {
            int32_t weighted_sum = 0;

            // Weighted sum: sum(attention_weights[i, k] * V[k, j])
            for (uint32_t k = 0; k < 64; k++) {
                weighted_sum += (int32_t)attention_weights[i * 64 + k] * (int32_t)V[k * 64 + j];
            }

            // Requantize to INT8 (divide by 127 since weights sum to ~127)
            weighted_sum >>= 7;  // Divide by 128 (close to 127)

            if (weighted_sum > 127) weighted_sum = 127;
            if (weighted_sum < -128) weighted_sum = -128;

            output[i * 64 + j] = (int8_t)weighted_sum;
        }
    }
}

/**
 * Simplified attention for testing: just Q @ K^T (no softmax)
 * Useful for validating the pipeline before adding complexity
 */
void attention_scores_only_64x64(
    const int8_t* Q,      // [64 x 64]
    const int8_t* K,      // [64 x 64]
    int8_t* scores,       // [64 x 64] output
    uint32_t scale_shift  // Scaling factor
) {
    for (uint32_t i = 0; i < 64; i++) {
        for (uint32_t j = 0; j < 64; j++) {
            int32_t score = 0;
            for (uint32_t k = 0; k < 64; k++) {
                score += (int32_t)Q[i * 64 + k] * (int32_t)K[j * 64 + k];
            }
            score >>= scale_shift;
            if (score > 127) score = 127;
            if (score < -128) score = -128;
            scores[i * 64 + j] = (int8_t)score;
        }
    }
}

/**
 * Multi-head attention (processes 2 heads in parallel) - COMBINED VERSION
 * Each head: 64x64 matrices
 *
 * QKV_combined_2heads layout:
 *   Head 0: Bytes 0-12287 (Q:0-4095, K:4096-8191, V:8192-12287)
 *   Head 1: Bytes 12288-24575 (Q:12288-16383, K:16384-20479, V:20480-24575)
 */
void multi_head_attention_2heads(
    const int8_t* QKV_combined_2heads,  // [2 × 12288] for 2 heads
    int8_t* output,                      // [2 × 64 × 64] outputs for 2 heads
    uint32_t scale_shift                 // Scaling factor
) {
    // Process head 0 (bytes 0-12287)
    attention_64x64(&QKV_combined_2heads[0], &output[0], scale_shift);

    // Process head 1 (bytes 12288-24575)
    attention_64x64(&QKV_combined_2heads[12288], &output[4096], scale_shift);
}
