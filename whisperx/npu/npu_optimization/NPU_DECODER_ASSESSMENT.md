# NPU Whisper Decoder Implementation - Assessment Report

**Date**: November 2, 2025
**Team Lead**: NPU Decoder Implementation Team
**Status**: Foundation Ready - Decoder Implementation Pending
**Target**: 220x realtime transcription (full encoder + decoder on NPU)

---

## Executive Summary

The Whisper decoder is **significantly more complex** than the encoder due to its autoregressive nature. While the encoder can process all frames in parallel, the decoder generates one token at a time in a sequential loop. This fundamental difference makes NPU acceleration **more challenging** but still **achievable** based on UC-Meeting-Ops proof-of-concept.

**Current Status**:
- ‚úÖ **Encoder**: 36.1x realtime on NPU (mel preprocessing complete)
- ‚úÖ **NPU Infrastructure**: 100% operational (XRT 2.20.0, MLIR-AIE2, Phoenix NPU)
- ‚úÖ **Encoder Kernels**: 4 working kernels (mel, matmul, attention, GELU)
- ‚ö†Ô∏è **Decoder**: Python prototype exists but not integrated with NPU kernels
- üéØ **Target**: 220x realtime (proven by UC-Meeting-Ops on same hardware)

**Key Finding**: Decoder complexity requires a **hybrid approach** initially, with gradual migration to full NPU implementation.

---

## 1. What Exists - Current Decoder Code

### 1.1 Decoder Python Prototype ‚úÖ

**File**: `/home/ucadmin/UC-1/Unicorn-Amanuensis/whisperx/npu/npu_optimization/whisper_encoder_kernels/whisper_npu_decoder_matmul.py`

**Size**: 561 lines
**Status**: Complete Python implementation, not yet connected to NPU kernels
**Components**:
- ‚úÖ `WhisperNPUSelfAttention` - Causal self-attention with NPU matmul layers
- ‚úÖ `WhisperNPUCrossAttention` - Cross-attention with encoder outputs
- ‚úÖ `WhisperNPUDecoderLayer` - Complete decoder layer (self-attn + cross-attn + FFN)
- ‚úÖ `WhisperNPUDecoderMatmul` - Full 6-layer decoder stack
- ‚ö†Ô∏è Uses `NPUMatmul` wrapper but **does NOT use custom XCLBIN kernels yet**

**Key Architecture**:
```python
class WhisperNPUDecoderMatmul:
    - 6 decoder layers (for base model)
    - Each layer:
        1. LayerNorm ‚Üí Self-Attention (causal) ‚Üí Residual
        2. LayerNorm ‚Üí Cross-Attention (with encoder) ‚Üí Residual
        3. LayerNorm ‚Üí FFN (fc1 + GELU + fc2) ‚Üí Residual
    - Total: 10 matmuls per layer √ó 6 layers = 60 NPU operations
```

**Performance Target** (from code comments):
```python
# Target: 25-29√ó realtime (from 19.1√ó baseline)
# Current encoder: 36.1x realtime
# Need decoder to match or exceed 25x
```

### 1.2 Decoder ONNX Models Available ‚úÖ

**Location**: `/home/ucadmin/UC-1/Unicorn-Amanuensis/whisperx/models/whisper_onnx_cache/models--onnx-community--whisper-base/onnx/`

**Files**:
- `decoder_model_int8.onnx` (51 MB) - INT8 quantized decoder **without** KV cache
- `decoder_with_past_model_int8.onnx` (48 MB) - INT8 decoder **with** KV cache optimization
- `decoder_model.onnx` (199 MB) - FP32 reference decoder
- Multiple quantization variants (fp16, q4, bnb4, uint8)

**Key Insight**: The existence of `decoder_with_past_model_int8.onnx` proves that **KV cache is critical** for efficient decoder inference. This is the model we should target.

### 1.3 NPU Integration Example ‚úÖ

**File**: `/home/ucadmin/UC-1/Unicorn-Amanuensis/whisperx/npu/npu_optimization/whisper_encoder_kernels/npu_whisper_integration_example.py`

**Status**: Working end-to-end pipeline demo
**Components**:
- `NPUWhisperPipeline` class
- Loads both encoder and decoder
- Demonstrates transcription flow
- **Limitation**: Uses random tokens for decoder input (placeholder implementation)

**Critical Code Section**:
```python
# Line 113-114: PLACEHOLDER IMPLEMENTATION
max_length = min(250, audio_features.size(0) // 2)
decoder_input = torch.randn(max_length, self.config["d_model"])
# ‚ö†Ô∏è This is NOT real text generation!
```

**What's Missing**:
- ‚ùå No tokenizer integration
- ‚ùå No autoregressive generation loop
- ‚ùå No beam search
- ‚ùå No KV cache management
- ‚ùå No stopping criteria (EOS token detection)

### 1.4 Existing NPU Kernels Ready for Decoder ‚úÖ

**Available XCLBINs**:

1. **Matrix Multiply Kernel** (`matmul_simple.xclbin`, 11 KB)
   - Status: Compiled, runs on NPU
   - Capability: 16√ó16 INT8 matmul
   - Performance: 1.18ms per operation (needs optimization)
   - **Use Case**: Q/K/V projections, FFN layers

2. **Attention Kernel** (`attention_simple.xclbin`, 12 KB)
   - Status: Working, validated on hardware
   - Capability: 16√ó16 scaled dot-product attention
   - Performance: 0.56ms per tile
   - **Use Case**: Self-attention and cross-attention mechanisms

3. **GELU Activation** (`gelu_simple.xclbin`, 9 KB)
   - Status: Perfect accuracy (MAE = 0.00)
   - Capability: 512 or 2048 element GELU
   - Performance: 0.32 ¬µs (projected NPU time)
   - **Use Case**: FFN activation in decoder layers

4. **Mel Spectrogram** (`mel_fixed_v3.xclbin`, 56 KB)
   - Status: Production ready (36.1x realtime)
   - **Use Case**: Audio preprocessing (not decoder, but proven pipeline)

**Reusability**: All encoder kernels can be reused in decoder with **minor modifications** for:
- Causal masking in self-attention
- Different buffer sizes for cross-attention

---

## 2. Decoder-Specific Challenges

### 2.1 Autoregressive Generation (CRITICAL)

**Problem**: Decoder generates **one token at a time**, creating a sequential bottleneck.

**Encoder vs Decoder Comparison**:

| Aspect | Encoder | Decoder |
|--------|---------|---------|
| **Processing Model** | Parallel - all frames at once | Sequential - one token per step |
| **Latency Sensitivity** | Can batch frames | Each token blocks the next |
| **NPU Utilization** | 100% with batching | Potential underutilization per step |
| **Optimization** | Easy - more frames = more work | Hard - single token = low work |

**Typical Whisper Generation Loop**:
```python
# Simplified autoregressive loop
tokens = [START_TOKEN]
for step in range(max_length):
    # 1. Get decoder input (past tokens + position embeddings)
    decoder_input = embed_tokens(tokens)

    # 2. Run decoder forward pass (EXPENSIVE: 6 layers √ó 3 operations)
    decoder_output = decoder(decoder_input, encoder_hidden_states, past_kv_cache)

    # 3. Project to vocabulary (EXPENSIVE: 512 ‚Üí 51865 matmul)
    logits = lm_head(decoder_output[-1])  # Last token only

    # 4. Sample next token (beam search / greedy / top-k)
    next_token = sample(logits)

    # 5. Check for EOS
    if next_token == EOS_TOKEN:
        break

    tokens.append(next_token)
```

**Performance Impact**:
- Each step: 6 decoder layers + vocabulary projection
- Typical output: 50-250 tokens for 30s audio
- **Serial dependency**: Cannot start token N+1 until token N is complete

**NPU Challenge**: How to keep NPU busy with single-token generation?

### 2.2 KV Cache Management (CRITICAL)

**What is KV Cache?**

During autoregressive generation, each decoder layer computes Key and Value matrices for self-attention. Without caching, we recompute these for **all previous tokens** at each step.

**Without KV Cache** (naive):
```
Step 1: Compute K,V for token 1                    ‚Üí 1 computation
Step 2: Compute K,V for tokens 1,2                 ‚Üí 2 computations
Step 3: Compute K,V for tokens 1,2,3               ‚Üí 3 computations
...
Step 50: Compute K,V for tokens 1,2,...,50         ‚Üí 50 computations
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 1+2+3+...+50 = 1,275 computations          (O(n¬≤))
```

**With KV Cache** (optimized):
```
Step 1: Compute K,V for token 1, cache            ‚Üí 1 computation
Step 2: Use cached K,V for token 1, compute token 2 ‚Üí 1 computation
Step 3: Use cached K,V, compute token 3            ‚Üí 1 computation
...
Step 50: Use cached K,V, compute token 50          ‚Üí 1 computation
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 50 computations                             (O(n))
```

**Speedup**: 25-50x faster with KV cache!

**NPU Implementation Challenges**:

1. **Memory Management**:
   - Cache size: `[batch, num_heads, seq_len, head_dim]`
   - For base model: `[1, 8, 250, 64]` = 128KB per layer √ó 6 layers = 768KB
   - Phoenix NPU tile memory: 32KB per compute core
   - **Solution**: Must use host memory or multi-tile coordination

2. **Cache Updates**:
   - Each step: Read old cache + Write new cache
   - Requires efficient DMA transfers
   - Must minimize CPU involvement

3. **Cross-Attention Cache**:
   - Encoder K,V is computed **once** and reused
   - Size: `[batch, num_heads, encoder_seq_len, head_dim]`
   - For 30s audio: ~1500 frames ‚Üí ~600KB
   - **Optimization opportunity**: Keep in NPU memory

**Design Question**: Where to store KV cache?
- **Option A**: Host memory (DDR) - Large capacity, slower access
- **Option B**: NPU memory tiles - Fast access, limited capacity
- **Option C**: Hybrid - Cross-attention on NPU, self-attention in host memory

### 2.3 Causal Masking (MODERATE)

**Requirement**: Decoder self-attention must be **causal** to prevent "looking ahead" during autoregressive generation.

**Causal Mask**:
```
Attention mask for 5 tokens:
[1 0 0 0 0]  ‚Üê Token 1 can only see itself
[1 1 0 0 0]  ‚Üê Token 2 can see tokens 1,2
[1 1 1 0 0]  ‚Üê Token 3 can see tokens 1,2,3
[1 1 1 1 0]  ‚Üê Token 4 can see tokens 1,2,3,4
[1 1 1 1 1]  ‚Üê Token 5 can see all tokens
```

**Implementation**:
```python
# CPU implementation (easy)
causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
scores = scores.masked_fill(causal_mask, float('-inf'))
```

**NPU Implementation**:
- Attention kernel already supports masking
- Need to generate causal mask pattern
- Apply during softmax computation
- **Complexity**: Low - already proven in encoder attention

### 2.4 Cross-Attention with Encoder (MODERATE)

**Requirement**: Decoder attends to encoder outputs at each layer.

**Architecture**:
```
Decoder Input (tokens) ‚îÄ‚îÄ‚îê
                          ‚îÇ
                          ‚ñº
                    Self-Attention (causal)
                          ‚îÇ
                          ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇCross-Attn   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ Encoder Hidden States (static)
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
                        FFN
                          ‚îÇ
                          ‚ñº
                    Decoder Output
```

**NPU Challenge**:
- Query (Q) comes from decoder
- Key (K) and Value (V) come from encoder
- Encoder outputs are **constant** across all decoder steps
- **Optimization**: Compute encoder K,V **once**, cache on NPU

**Memory Requirements**:
- Encoder output: `[1500 frames, 512 dims]` = 768KB (FP32) or 192KB (INT8)
- Fits in NPU L3 memory or host memory
- Should stay resident during entire decoding

### 2.5 Vocabulary Projection (CRITICAL)

**Requirement**: Map decoder output to vocabulary logits for token prediction.

**Operation**:
```python
# Decoder output: [seq_len, d_model] = [250, 512]
# Vocabulary weights: [vocab_size, d_model] = [51865, 512]
# Logits: [seq_len, vocab_size] = [250, 51865]
logits = decoder_output @ vocab_weights.T
```

**Performance Impact**:
- **Largest matmul in the entire model**: 512 ‚Üí 51,865 dimensions
- Current NPU matmul: 16√ó16 tiles = 256 elements
- Need ~200 tiles per output token
- At 1.18ms per tile: ~236ms per token = **BOTTLENECK**

**Optimization Strategies**:

1. **INT8 Quantization**:
   - Reduce from FP32 to INT8
   - 4x faster matmul
   - **Target**: ~60ms per token

2. **Sparse Vocabulary**:
   - Don't compute all 51,865 logits
   - Use top-K vocabulary subset (~5,000 most common tokens)
   - **Speedup**: 10x reduction ‚Üí 6ms per token ‚úÖ

3. **Kernel Fusion**:
   - Combine decoder output + vocabulary projection
   - Eliminate intermediate transfers
   - **Speedup**: 20-30% improvement

4. **Beam Search Batching**:
   - Process multiple beams simultaneously
   - Better NPU utilization
   - **Complexity**: Requires careful memory management

**Recommendation**: Start with sparse vocabulary (top-5K tokens) to avoid bottleneck.

### 2.6 Beam Search on NPU (ADVANCED)

**Beam Search Algorithm**:
```
Beam Width = 5

Step 1:
  Start: [START]
  Expand: 5 candidate sequences

Step 2:
  Expand each of 5 sequences ‚Üí 25 candidates
  Keep top 5 by score

Step 3:
  Expand each of 5 sequences ‚Üí 25 candidates
  Keep top 5 by score

... continue until EOS or max length
```

**NPU Implementation Challenges**:

1. **Branching Logic**:
   - Beam search requires **dynamic** branching
   - NPU kernels are designed for **static** compute
   - **Solution**: CPU coordinates beams, NPU does compute

2. **Score Computation**:
   - Need log-probabilities (softmax + log)
   - Requires reduction across 51,865 vocabulary
   - **Solution**: Compute top-K on CPU after NPU matmul

3. **Memory Management**:
   - Track KV cache for each beam
   - Beam width = 5 ‚Üí 5√ó memory usage
   - **Constraint**: May exceed NPU memory

**Recommendation**:
- **Phase 1**: Greedy decoding (beam_size=1) on NPU
- **Phase 2**: CPU-coordinated beam search with NPU compute
- **Phase 3**: Investigate NPU-native beam search (if needed)

---

## 3. Current Baseline Performance

### 3.1 CPU-Only Decoder Performance

**Measured Performance** (from server_dynamic.py):
- **Speed**: 13.5x realtime (faster-whisper INT8)
- **CPU Usage**: 0.24% (very efficient)
- **Accuracy**: Perfect (2.5% WER on test data)
- **Backend**: CTranslate2 with INT8 quantization

**Time Breakdown** (estimated for 30s audio):
```
Total Processing: ~2.2 seconds
  - Audio Loading: 0.05s (2%)
  - Mel Spectrogram: 0.10s (5%) ‚Üê Already 36x on NPU ‚úÖ
  - Encoder: 0.80s (36%)
  - Decoder: 1.20s (55%)
  - Post-processing: 0.05s (2%)
```

**Decoder Bottleneck Analysis**:
- Decoder: 1.20s for 30s audio = 25x realtime
- Target: 220x realtime
- **Required speedup**: 8.8x improvement

### 3.2 Hybrid NPU Performance (Current)

**File**: `npu/npu_optimization/onnx_whisper_npu.py`

**Status**: ‚ö†Ô∏è Experimental, garbled output

**Reported Performance**:
- **Speed**: 10.7x realtime (slower than CPU!)
- **CPU Usage**: 15-20%
- **Accuracy**: ‚ùå Garbled output (decoder issues)

**Issues Identified**:
1. **Decoder Output**: Limited to 20 tokens per generation
2. **KV Cache**: Missing proper implementation
3. **NPU Kernels**: Not actually used (ONNX Runtime uses CPUExecutionProvider)
4. **Token Sequence**: Incorrect configuration

**Conclusion**: This is a placeholder implementation, not production-ready.

### 3.3 Target Performance (220x Realtime)

**Reference**: UC-Meeting-Ops achieved 220x on identical hardware (AMD Phoenix NPU)

**Projected Breakdown** (for 30s audio):
```
Total Processing: ~0.136 seconds (220x realtime)
  - Mel Spectrogram: 0.020s (NPU) ‚úÖ Already proven at 36x
  - Encoder: 0.040s (NPU)
  - Decoder: 0.070s (NPU)
  - Post-processing: 0.006s (CPU)
```

**Decoder Performance Target**:
- Current CPU: 1.20s
- Target NPU: 0.070s
- **Required speedup**: 17x improvement

**Feasibility**: ‚úÖ Achievable based on UC-Meeting-Ops proof-of-concept

---

## 4. NPU Infrastructure Readiness

### 4.1 Hardware ‚úÖ READY

**AMD Phoenix NPU (XDNA1)**:
- **Device**: `/dev/accel/accel0` - Accessible ‚úÖ
- **Tile Array**: 4√ó6 (4 columns √ó 6 rows)
  - Row 2: 4√ó AIE-ML compute cores
  - Row 1: 4√ó Memory tiles (32KB each)
  - Row 0: 4√ó ShimNOC (DMA + host interface)
- **Performance**: 10-15 TOPS INT8 (sustained)
- **Power**: 5-10W (vs 45W CPU, 125W GPU)
- **Status**: Fully operational, tested with 4 working kernels

### 4.2 Software Stack ‚úÖ READY

**XRT 2.20.0**:
- **Installation**: Complete ‚úÖ
- **NPU Plugin**: `xrt_plugin-amdxdna_2.20_amd64.deb` installed
- **Firmware**: 1.5.5.391 (latest)
- **Python Bindings**: PyXRT working
- **Status**: Production-ready

**MLIR-AIE2 Toolchain**:
- **Version**: v1.1.1 (official release)
- **Tools**: `aie-opt`, `aie-translate`, `aiecc.py`
- **Peano Compiler**: C++ AIE2 compiler operational
- **Compilation**: <1 second per kernel
- **Status**: Proven with 4 compiled XCLBINs

### 4.3 Working Encoder Kernels ‚úÖ READY

**Compiled XCLBINs** (can be reused for decoder):

1. **Matrix Multiply** (`matmul_simple.xclbin`, 11 KB)
   - Tile size: 16√ó16 INT8
   - Performance: 1.18ms (needs optimization)
   - **Decoder use**: Q/K/V projections, FFN layers

2. **Attention** (`attention_simple.xclbin`, 12 KB)
   - Tile size: 16√ó16
   - Performance: 0.56ms
   - **Decoder use**: Self-attention, cross-attention

3. **GELU** (`gelu_simple.xclbin`, 9 KB)
   - Elements: 512 or 2048
   - Accuracy: Perfect (MAE = 0.00)
   - **Decoder use**: FFN activation

4. **Mel Spectrogram** (`mel_fixed_v3.xclbin`, 56 KB)
   - Performance: 36.1x realtime
   - **Decoder use**: Preprocessing (already working)

**Reusability**: ~80% of code can be shared between encoder and decoder kernels.

**Modifications Needed for Decoder**:
- Causal masking in self-attention kernel
- Cross-attention input handling
- KV cache buffer management

### 4.4 Integration Framework ‚úÖ READY

**NPUMatmul Wrapper** (`npu_matmul_wrapper.py`):
- XRT device management
- Buffer allocation
- DMA transfers
- Kernel execution
- **Status**: Working in encoder

**Integration Example** (`npu_whisper_integration_example.py`):
- End-to-end pipeline demo
- Encoder + Decoder coordination
- **Status**: Encoder working, decoder needs real implementation

---

## 5. Critical Findings

### 5.1 Decoder is NOT a Simple Extension of Encoder

**Key Differences**:

| Aspect | Encoder | Decoder |
|--------|---------|---------|
| **Processing** | Parallel (all frames) | Sequential (token-by-token) |
| **Latency** | Batch processing hides latency | Each token adds latency |
| **Memory** | Static (mel features) | Dynamic (growing KV cache) |
| **Attention** | Self-attention only | Self-attention + Cross-attention |
| **Output** | Fixed-size hidden states | Variable-length token sequence |
| **Optimization** | More frames = better NPU utilization | Single token = potential underutilization |

**Conclusion**: Decoder requires fundamentally different optimization strategies.

### 5.2 KV Cache is Make-or-Break

**Performance Impact**:
- **Without KV cache**: O(n¬≤) complexity ‚Üí 25-50x slower
- **With KV cache**: O(n) complexity ‚Üí Required for 220x target

**Implementation Complexity**:
- Requires 768KB cache (base model, 250 tokens)
- Must coordinate across 6 decoder layers
- Needs efficient NPU ‚Üî host memory management

**Recommendation**: **MUST** implement KV cache from day 1, not an "optimization".

### 5.3 Vocabulary Projection is a Bottleneck

**Scale of the Problem**:
- Vocabulary size: 51,865 tokens
- Matrix multiply: `[512] √ó [512, 51865]` = 26M operations per token
- With current NPU matmul (1.18ms/tile): ~236ms per token
- **Bottleneck**: Would limit to 4 tokens/second ‚Üí **UNACCEPTABLE**

**Mitigation Strategies**:
1. **Sparse vocabulary** (top-5K tokens): 10x speedup ‚Üí 23ms/token ‚úÖ
2. **INT8 quantization**: 4x speedup ‚Üí 6ms/token ‚úÖ
3. **Kernel fusion**: Additional 1.3x ‚Üí 4ms/token ‚úÖ
4. **Multi-core NPU**: 4√ó parallel ‚Üí 1ms/token ‚úÖ

**Conclusion**: With optimizations, vocabulary projection is **solvable** but requires careful design.

### 5.4 Hybrid Approach is Necessary Initially

**Recommendation**: Start with **hybrid NPU/CPU** approach:

**Phase 1 - Hybrid**:
- NPU: Encoder (proven working)
- NPU: Decoder matmul operations
- CPU: Autoregressive loop coordination
- CPU: Token sampling and beam search
- **Target**: 40-60x realtime

**Phase 2 - Mostly NPU**:
- NPU: Encoder
- NPU: Decoder layers
- NPU: KV cache management
- CPU: Minimal coordination only
- **Target**: 100-150x realtime

**Phase 3 - Full NPU**:
- NPU: Entire pipeline
- CPU: Only host-device transfers
- **Target**: 200-220x realtime

**Rationale**: Proven approach used by UC-Meeting-Ops.

### 5.5 Greedy Decoding Before Beam Search

**Recommendation**: Implement in this order:

1. **Greedy Decoding** (beam_size=1):
   - Simplest to implement
   - No branching logic
   - Minimal memory overhead
   - **Target**: 80-100x realtime

2. **Beam Search** (beam_size=5):
   - Better accuracy
   - 5√ó memory overhead
   - Requires careful coordination
   - **Target**: 200-220x realtime

**Rationale**: Get single-beam working perfectly first, then scale to multi-beam.

---

## 6. Recommendations

### 6.1 Immediate Priorities (Week 1-2)

1. **Fix Existing Decoder Issues** (2-3 days):
   - Debug garbled output in `onnx_whisper_npu.py`
   - Implement proper KV cache
   - Extend token generation limit
   - **Deliverable**: Accurate transcription (even if slow)

2. **Integrate NPU Kernels with Decoder** (2-3 days):
   - Connect `whisper_npu_decoder_matmul.py` to XCLBIN kernels
   - Test matmul, attention, GELU on decoder operations
   - Measure per-token latency
   - **Deliverable**: First decoder operation on NPU

3. **Implement Causal Masking** (1-2 days):
   - Modify attention kernel for causal mask
   - Test autoregressive generation
   - **Deliverable**: Working causal self-attention

### 6.2 Short-term Goals (Week 3-4)

4. **Optimize Vocabulary Projection** (2-3 days):
   - Implement top-K sparse vocabulary
   - Fuse decoder output + projection kernel
   - **Deliverable**: Sub-10ms vocabulary projection

5. **Implement KV Cache on NPU** (3-4 days):
   - Design memory layout
   - Implement cache updates
   - Test with growing sequences
   - **Deliverable**: O(n) decoder complexity

6. **End-to-End Greedy Decoding** (2-3 days):
   - Integrate all components
   - Test on real audio
   - Measure accuracy and performance
   - **Deliverable**: 40-60x realtime with greedy decoding

### 6.3 Medium-term Goals (Week 5-8)

7. **Scale Tile Sizes** (1-2 weeks):
   - Move from 16√ó16 to 64√ó64 tiles
   - Optimize memory layout
   - **Deliverable**: 3-4x latency reduction

8. **Implement Beam Search** (1-2 weeks):
   - CPU-coordinated multi-beam
   - NPU parallel compute
   - **Deliverable**: Production-quality transcription

9. **Multi-Core Optimization** (1 week):
   - Use all 4 Phoenix NPU cores
   - Pipeline operations
   - **Deliverable**: 200-220x realtime target achieved

### 6.4 Success Metrics

**Phase 1** (Week 2):
- ‚úÖ Decoder produces accurate text (no garbled output)
- ‚úÖ At least one NPU kernel running in decoder
- **Target**: 20-30x realtime

**Phase 2** (Week 4):
- ‚úÖ KV cache working
- ‚úÖ Greedy decoding on NPU
- **Target**: 60-80x realtime

**Phase 3** (Week 6):
- ‚úÖ Vocabulary projection optimized
- ‚úÖ Full decoder layers on NPU
- **Target**: 120-150x realtime

**Phase 4** (Week 8):
- ‚úÖ Beam search implemented
- ‚úÖ Multi-core optimization
- **Target**: 200-220x realtime ‚ú®

---

## 7. Risk Assessment

### 7.1 High Risk

**Risk**: Sequential decoder bottleneck prevents NPU from reaching full utilization.

**Mitigation**:
- Implement batching across multiple audio streams
- Pipeline token generation with encoder
- Use all 4 NPU cores in parallel

**Probability**: Medium
**Impact**: High
**Status**: Requires careful design

### 7.2 Medium Risk

**Risk**: KV cache doesn't fit in NPU memory, requires host memory transfers.

**Mitigation**:
- Use hybrid approach (cross-attention in NPU, self-attention in host)
- Optimize DMA transfers
- Compress cache with quantization

**Probability**: High
**Impact**: Medium (adds latency but still faster than CPU)
**Status**: Solvable with engineering

### 7.3 Low Risk

**Risk**: Decoder accuracy degrades with INT8 quantization.

**Mitigation**:
- Use INT8 for matmuls, FP16 for softmax
- Calibrate quantization ranges carefully
- Fall back to FP16 if needed

**Probability**: Low
**Impact**: Medium
**Status**: Well-understood problem

---

## 8. Conclusion

**Key Findings**:
1. ‚úÖ **Infrastructure is ready**: NPU hardware, XRT, MLIR-AIE2 all working
2. ‚úÖ **Encoder kernels proven**: Can be reused for decoder
3. ‚ö†Ô∏è **Decoder complexity is high**: Autoregressive nature requires careful design
4. ‚úÖ **220x target is achievable**: Proven by UC-Meeting-Ops on same hardware
5. üîß **Hybrid approach recommended**: Start CPU-coordinated, migrate to full NPU

**Bottom Line**:
The decoder is **more challenging** than the encoder but **definitely achievable**. The key is to take a **phased approach**:
1. Get accuracy working first (fix garbled output)
2. Add NPU kernels incrementally (matmul ‚Üí attention ‚Üí full layers)
3. Optimize sequentially (KV cache ‚Üí vocabulary ‚Üí multi-core)

**Timeline Estimate**: 8-10 weeks to reach 220x target, with incremental value delivered every 2 weeks.

**Confidence Level**: **HIGH** - All foundational components are proven and ready.

---

**Report Prepared By**: NPU Decoder Implementation Team Lead
**Date**: November 2, 2025
**Next Steps**: Proceed to detailed design document (NPU_DECODER_DESIGN.md)
