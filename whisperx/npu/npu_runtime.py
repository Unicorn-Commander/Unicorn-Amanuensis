#!/usr/bin/env python3
"""
NPU Runtime using custom MLIR-AIE2 kernels and direct NPU access
Integrates with our custom AIE2 kernel driver
"""

import os
import numpy as np
import logging
from typing import Dict, Any, Optional, Union
from pathlib import Path
import time
import wave

logger = logging.getLogger(__name__)

# Check for CPU-only mode
CPU_ONLY_MODE = os.environ.get('CPU_ONLY_MODE', '').lower() in ('1', 'true', 'yes')

class NPURuntime:
    """NPU Runtime using custom MLIR-AIE2 kernels"""

    def __init__(self):
        if CPU_ONLY_MODE:
            logger.info("ðŸ–¥ï¸ NPURuntime: Running in CPU-only mode")
            self.available = False
            self._runtime = None
        else:
            self._runtime = CustomAIE2Runtime()
            self.available = self._runtime.is_available()

    def is_available(self) -> bool:
        return self.available

    def load_model(self, model_path: str) -> bool:
        if not self.available:
            return False
        return self._runtime.load_model(model_path)

    def transcribe(self, audio_data) -> Dict[str, Any]:
        if not self.available:
            return {"error": "NPU not available", "text": "", "npu_accelerated": False}
        return self._runtime.transcribe(audio_data)

    def get_device_info(self) -> Dict[str, Any]:
        if not self.available or not self._runtime:
            return {"status": "not available"}
        return self._runtime.get_device_info()

class CustomAIE2Runtime:
    """Runtime that uses proven ONNX Whisper + NPU hybrid approach"""

    def __init__(self):
        self.device_path = "/dev/accel/accel0"
        self.available = False
        self.model_loaded = False
        self.aie2_driver = None
        self.direct_runtime = None
        self.onnx_whisper_npu = None
        self.aie_version = "2.0"  # Phoenix NPU is AIE2 (XDNA1)

        # Try to load proven ONNX Whisper NPU system FIRST
        try:
            from npu_optimization.onnx_whisper_npu import ONNXWhisperNPU
            self.onnx_whisper_npu = ONNXWhisperNPU()
            logger.info("âœ… ONNX Whisper + NPU system loaded (proven 51x realtime approach)")
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load ONNX Whisper NPU: {e}")

        # Try to load direct NPU runtime (XRT-based) as fallback
        try:
            from npu_optimization.direct_npu_runtime import DirectNPURuntime
            self.direct_runtime = DirectNPURuntime()
            if self.direct_runtime.initialize():
                logger.info("âœ… Direct NPU runtime (XRT) initialized")
            else:
                logger.warning("âš ï¸ XRT NPU runtime initialization failed")
                self.direct_runtime = None
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load XRT runtime: {e}")

        # Try to load custom AIE2 kernel driver (pass direct_runtime to it)
        try:
            from npu_optimization.aie2_kernel_driver import AIE2KernelDriver
            self.aie2_driver = AIE2KernelDriver(direct_runtime=self.direct_runtime)
            logger.info("âœ… AIE2 Kernel Driver loaded")
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load AIE2 driver: {e}")

        # Check device availability
        if Path(self.device_path).exists():
            self.available = True
            logger.info(f"âœ… NPU device found: {self.device_path}")
        else:
            logger.warning(f"âš ï¸ NPU device not found: {self.device_path}")

    def is_available(self) -> bool:
        return self.available

    def open_device(self) -> bool:
        """Open NPU device - compatibility method for npu_accelerator.py"""
        # XRT device is already opened in __init__ via direct_runtime.initialize()
        if self.direct_runtime and self.direct_runtime.is_initialized:
            logger.info("âœ… NPU device already opened via XRT")
            return True
        elif self.available and Path(self.device_path).exists():
            # Try to initialize XRT runtime now
            try:
                from npu_optimization.direct_npu_runtime import DirectNPURuntime
                self.direct_runtime = DirectNPURuntime()
                if self.direct_runtime.initialize():
                    logger.info("âœ… XRT NPU device opened successfully")
                    return True
            except Exception as e:
                logger.error(f"âŒ Failed to open NPU device: {e}")
        return False

    def load_model(self, model_path: str) -> bool:
        """Load ONNX Whisper models for NPU inference using proven approach"""
        try:
            logger.info(f"ðŸ”„ Loading model for NPU: {model_path}")

            # Use proven ONNX Whisper NPU approach
            if self.onnx_whisper_npu:
                # Determine model size from path
                model_size = "base"  # Default
                if "large" in model_path.lower():
                    model_size = "large"
                elif "medium" in model_path.lower():
                    model_size = "medium"
                elif "small" in model_path.lower():
                    model_size = "small"
                elif "tiny" in model_path.lower():
                    model_size = "tiny"

                logger.info(f"ðŸš€ Initializing proven ONNX Whisper + NPU system (model: {model_size})")
                logger.info("   This is the 51x realtime approach from whisper_npu_project")

                if self.onnx_whisper_npu.initialize(model_size=model_size):
                    self.model_loaded = True
                    logger.info("âœ… ONNX Whisper + NPU initialized successfully!")
                    logger.info("   Expected performance: 25-51x realtime")
                    return True
                else:
                    logger.error("âŒ ONNX Whisper + NPU initialization failed")
                    return False
            else:
                logger.error("âŒ ONNX Whisper NPU system not loaded")
                return False

        except Exception as e:
            logger.error(f"âŒ Model loading error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def transcribe(self, audio_data: Union[np.ndarray, bytes, str]) -> Dict[str, Any]:
        """Transcribe using proven ONNX Whisper + NPU approach"""
        if not self.model_loaded:
            return {"error": "Model not loaded", "text": "", "npu_accelerated": False}

        try:
            start_time = time.time()

            # Handle different audio input types
            if isinstance(audio_data, str) and os.path.exists(audio_data):
                # Audio file path - can pass directly to ONNX Whisper NPU
                audio_path = audio_data
                logger.info(f"ðŸ“ Loading audio from: {audio_path}")
            else:
                # Need to convert to file for ONNX Whisper NPU
                import tempfile
                if isinstance(audio_data, bytes):
                    audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
                else:
                    audio_array = audio_data.astype(np.float32)

                # Save to temp file
                temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                audio_path = temp_file.name
                temp_file.close()

                # Write WAV file
                import scipy.io.wavfile as wavfile
                wavfile.write(audio_path, 16000, (audio_array * 32768).astype(np.int16))
                logger.info(f"ðŸ“ Saved audio to temp file: {audio_path}")
                should_cleanup = True

            # Use proven ONNX Whisper + NPU approach
            if self.onnx_whisper_npu and self.onnx_whisper_npu.is_ready:
                logger.info("ðŸš€ Using proven ONNX Whisper + NPU system (51x realtime)")
                result = self.onnx_whisper_npu.transcribe_audio(audio_path)

                # Cleanup temp file if needed
                if 'should_cleanup' in locals() and should_cleanup:
                    try:
                        os.unlink(audio_path)
                    except:
                        pass

                return result
            else:
                logger.error("âŒ ONNX Whisper NPU not initialized")
                return {"error": "ONNX Whisper NPU not ready", "text": "", "npu_accelerated": False}

        except Exception as e:
            logger.error(f"âŒ Transcription error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "error": str(e),
                "text": f"Transcription failed: {str(e)}",
                "npu_accelerated": False
            }

    def get_device_info(self) -> Dict[str, Any]:
        """Get NPU device information"""
        return {
            "device_path": self.device_path,
            "device_available": self.available,
            "model_loaded": self.model_loaded,
            "aie2_driver": bool(self.aie2_driver),
            "direct_runtime": bool(self.direct_runtime),
            "mlir_kernels": "MLIR-AIE2" if self.aie2_driver else "None",
            "status": "operational" if self.model_loaded else "waiting"
        }

# Alias for compatibility
SimplifiedNPURuntime = CustomAIE2Runtime
RealNPURuntime = CustomAIE2Runtime
