#!/usr/bin/env python3
"""
ðŸ¦„ Unicorn Amanuensis - Smart Dynamic Server
Auto-detects hardware (NPU > iGPU > CPU) and models
Uses the best available acceleration automatically
"""

import os
import sys
import logging
import subprocess
from pathlib import Path
from typing import Optional, Dict
import tempfile
import time

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="ðŸ¦„ Unicorn Amanuensis (Dynamic)",
    description="Auto-detects best hardware: NPU â†’ iGPU â†’ CPU",
    version="3.0.0"
)

# Add CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
static_dir = Path(__file__).parent / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

class DynamicHardwareDetector:
    """Smart hardware detection with priority: NPU > iGPU > CPU"""

    @staticmethod
    def find_whisper_models() -> Dict[str, Path]:
        """Auto-find Whisper models in common locations"""
        search_paths = [
            Path("/home/ucadmin/UC-1/Unicorn-Amanuensis/whisperx/models/whisper_onnx_cache"),
            Path("/home/ucadmin/Development/whisper_npu_project/whisper_onnx_cache"),
            Path("/app/models/whisper_onnx_cache"),
            Path("/models/whisper_onnx_cache"),
            Path.home() / ".cache" / "whisper",
            Path.home() / ".cache" / "huggingface" / "hub",
        ]

        models = {}
        for search_path in search_paths:
            if search_path.exists():
                logger.info(f"ðŸ“ Scanning: {search_path}")
                for model_dir in search_path.iterdir():
                    if model_dir.is_dir() and "whisper" in model_dir.name.lower():
                        model_name = model_dir.name
                        models[model_name] = model_dir
                        logger.info(f"   âœ“ Found: {model_name}")

        return models

    @staticmethod
    def detect_npu() -> Optional[Dict]:
        """Detect AMD Phoenix NPU"""
        try:
            if not Path("/dev/accel/accel0").exists():
                return None

            result = subprocess.run(
                ["/opt/xilinx/xrt/bin/xrt-smi", "examine"],
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.returncode == 0 and "NPU Phoenix" in result.stdout:
                logger.info("ðŸš€ AMD Phoenix NPU detected!")

                # Count XCLBIN kernels
                kernel_dir = Path(__file__).parent / "npu/npu_optimization/whisper_encoder_kernels"
                kernel_count = len(list(kernel_dir.glob("*.xclbin"))) if kernel_dir.exists() else 0

                return {
                    "type": "npu",
                    "name": "AMD Phoenix NPU",
                    "device": "/dev/accel/accel0",
                    "kernels": kernel_count,
                    "priority": 1,  # Highest priority
                    "expected_speedup": "28-220x"
                }
        except Exception as e:
            logger.debug(f"NPU detection failed: {e}")

        return None

    @staticmethod
    def detect_igpu() -> Optional[Dict]:
        """Detect Intel iGPU"""
        try:
            result = subprocess.run(
                ["lspci"], capture_output=True, text=True
            )

            if "Intel" in result.stdout and ("VGA" in result.stdout or "Display" in result.stdout):
                logger.info("ðŸŽ¨ Intel iGPU detected!")
                return {
                    "type": "igpu",
                    "name": "Intel Integrated Graphics",
                    "priority": 2,
                    "expected_speedup": "13-19x"
                }
        except Exception as e:
            logger.debug(f"iGPU detection failed: {e}")

        return None

    @staticmethod
    def detect_hardware() -> Dict:
        """Detect best available hardware"""
        logger.info("ðŸ” Scanning hardware...")

        # Check in priority order
        npu = DynamicHardwareDetector.detect_npu()
        if npu:
            return npu

        igpu = DynamicHardwareDetector.detect_igpu()
        if igpu:
            return igpu

        logger.info("ðŸ’» Using CPU (fallback)")
        return {
            "type": "cpu",
            "name": "CPU",
            "priority": 3,
            "expected_speedup": "5-13x"
        }

class DynamicWhisperEngine:
    """Whisper engine that uses best available hardware"""

    def __init__(self):
        self.hardware = DynamicHardwareDetector.detect_hardware()
        self.models = DynamicHardwareDetector.find_whisper_models()
        self.engine = None
        self.current_model = "base"

        logger.info(f"âœ… Hardware selected: {self.hardware['name']}")
        logger.info(f"âœ… Models found: {len(self.models)}")

        self._initialize_engine()

    def _initialize_engine(self):
        """Initialize transcription engine based on hardware"""

        if self.hardware["type"] == "npu":
            self._init_npu_engine()
        elif self.hardware["type"] == "igpu":
            self._init_igpu_engine()
        else:
            self._init_cpu_engine()

    def _init_npu_engine(self):
        """Initialize NPU-accelerated engine"""
        try:
            sys.path.insert(0, str(Path(__file__).parent / 'npu'))
            from npu_runtime_unified import UnifiedNPURuntime

            self.npu_runtime = UnifiedNPURuntime()
            logger.info(f"âœ… NPU Runtime initialized!")
            logger.info(f"   Mel: {self.npu_runtime.mel_available}")
            logger.info(f"   GELU: {self.npu_runtime.gelu_available}")
            logger.info(f"   Attention: {self.npu_runtime.attention_available}")

            # For now, use faster-whisper as backend (will be NPU-accelerated in phase 2)
            from faster_whisper import WhisperModel
            self.engine = WhisperModel("base", device="cpu", compute_type="int8")
            logger.info("âœ… Using faster-whisper with NPU preprocessing")

        except Exception as e:
            logger.error(f"NPU init failed: {e}, falling back to CPU")
            self._init_cpu_engine()

    def _init_igpu_engine(self):
        """Initialize iGPU-accelerated engine"""
        try:
            from faster_whisper import WhisperModel
            self.engine = WhisperModel("base", device="cpu", compute_type="int8")
            logger.info("âœ… Using faster-whisper (iGPU mode planned)")
        except Exception as e:
            logger.error(f"iGPU init failed: {e}")
            self._init_cpu_engine()

    def _init_cpu_engine(self):
        """Initialize CPU engine"""
        try:
            from faster_whisper import WhisperModel
            self.engine = WhisperModel("base", device="cpu", compute_type="int8")
            logger.info("âœ… Using faster-whisper (CPU)")
        except Exception as e:
            logger.error(f"Engine init failed: {e}")
            raise

    async def transcribe(self, audio_path: str, model: str = "base") -> Dict:
        """Transcribe audio using best available hardware"""
        start_time = time.time()

        # Use NPU mel preprocessing if available
        mel_time = 0
        if hasattr(self, 'npu_runtime') and self.npu_runtime.mel_available:
            try:
                mel_start = time.time()
                logger.info("ðŸš€ Using NPU mel preprocessing...")

                # Load audio - convert with FFmpeg if needed
                import librosa
                import subprocess
                from pathlib import Path

                # Check if file needs conversion
                audio_ext = Path(audio_path).suffix.lower()
                if audio_ext in ['.m4a', '.mp4', '.aac', '.opus']:
                    # Convert to WAV using FFmpeg
                    wav_path = audio_path.replace(audio_ext, '.wav')

                    # Try to find ffmpeg
                    import shutil
                    ffmpeg_path = shutil.which('ffmpeg') or '/usr/bin/ffmpeg'

                    subprocess.run([
                        ffmpeg_path, '-i', audio_path,
                        '-ar', '16000',  # Resample to 16kHz
                        '-ac', '1',       # Convert to mono
                        '-y',             # Overwrite
                        wav_path
                    ], check=True, capture_output=True)
                    audio, sr = librosa.load(wav_path, sr=16000)
                else:
                    audio, sr = librosa.load(audio_path, sr=16000)

                # Process with NPU - call the mel_processor directly
                mel_features = self.npu_runtime.mel_processor.process(audio)
                mel_time = time.time() - mel_start
                logger.info(f"âœ… NPU mel completed in {mel_time:.3f}s - Shape: {mel_features.shape}")

            except Exception as e:
                logger.error(f"âŒ NPU mel preprocessing failed: {e}")
                raise HTTPException(status_code=500, detail=f"NPU preprocessing failed: {str(e)}. NPU-only mode - no CPU fallback allowed.")

        segments, info = self.engine.transcribe(
            audio_path,
            beam_size=5,
            language="en",
            vad_filter=False,
            word_timestamps=True
        )

        result_segments = []
        full_text = ""

        for segment in segments:
            segment_data = {
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            }

            if hasattr(segment, 'words'):
                segment_data["words"] = [
                    {"word": w.word, "start": w.start, "end": w.end}
                    for w in segment.words
                ]

            result_segments.append(segment_data)
            full_text += segment.text + " "

        elapsed = time.time() - start_time
        audio_duration = info.duration
        realtime_factor = audio_duration / elapsed if elapsed > 0 else 0

        return {
            "text": full_text.strip(),
            "segments": result_segments,
            "language": info.language,
            "duration": audio_duration,
            "processing_time": elapsed,
            "realtime_factor": f"{realtime_factor:.1f}x",
            "hardware": self.hardware["name"],
            "npu_mel_time": mel_time if mel_time > 0 else None
        }

# Initialize engine
logger.info("ðŸ¦„ Initializing Unicorn Amanuensis...")
whisper_engine = DynamicWhisperEngine()
logger.info("âœ… Server ready!")

@app.get("/")
async def root():
    return {
        "service": "Unicorn Amanuensis (Dynamic)",
        "version": "3.0.0",
        "hardware": whisper_engine.hardware,
        "models_found": len(whisper_engine.models),
        "status": "ready",
        "endpoints": {
            "/transcribe": "POST - Upload audio file",
            "/status": "GET - Server status",
            "/web": "GET - Web interface"
        }
    }

@app.get("/status")
async def status():
    return {
        "status": "ready",
        "hardware": whisper_engine.hardware,
        "models_found": list(whisper_engine.models.keys()),
        "current_model": whisper_engine.current_model,
        "npu_runtime": {
            "available": hasattr(whisper_engine, 'npu_runtime'),
            "mel_ready": whisper_engine.npu_runtime.mel_available if hasattr(whisper_engine, 'npu_runtime') else False,
            "gelu_ready": whisper_engine.npu_runtime.gelu_available if hasattr(whisper_engine, 'npu_runtime') else False,
            "attention_ready": whisper_engine.npu_runtime.attention_available if hasattr(whisper_engine, 'npu_runtime') else False,
        }
    }

@app.get("/web", response_class=HTMLResponse)
async def web_interface():
    """Serve web interface"""
    index_file = static_dir / "index.html"
    if index_file.exists():
        return FileResponse(index_file)
    else:
        return HTMLResponse("<h1>ðŸ¦„ Unicorn Amanuensis (Dynamic)</h1><p>Upload audio to transcribe</p>")

@app.post("/transcribe")
@app.post("/v1/audio/transcriptions")
async def transcribe(
    file: UploadFile = File(...),
    model: str = Form("base"),
    language: str = Form("en")
):
    """Transcribe audio file (supports both /transcribe and OpenAI-compatible /v1/audio/transcriptions)"""

    # Save uploaded file
    suffix = Path(file.filename).suffix if file.filename else ".wav"
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name

    try:
        result = await whisper_engine.transcribe(tmp_path, model)
        return JSONResponse(result)

    except Exception as e:
        logger.error(f"Transcription failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

    finally:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=9004)
