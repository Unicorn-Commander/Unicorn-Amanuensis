#!/usr/bin/env python3
"""
ü¶Ñ Unicorn Amanuensis - Docker-Ready Production Server
Production server that works in containers without external dependencies
"""

import os
import sys
import tempfile
import logging
import subprocess
from pathlib import Path
import time
import asyncio
from typing import Optional, Dict, List, AsyncGenerator

# Basic environment setup
os.environ.setdefault("PYTHONPATH", "/app")

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="ü¶Ñ Unicorn Amanuensis Production",
    description="Intel iGPU accelerated transcription service",
    version="2.0.0"
)

# Add CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
static_dir = Path(__file__).parent / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# Configuration
API_PORT = int(os.environ.get("API_PORT", "8000"))
DEFAULT_MODEL = os.environ.get("WHISPER_MODEL", "base")

# Global whisper instance
whisper_engine: Optional[object] = None

class DockerWhisperEngine:
    """Docker-compatible Whisper engine with fallback modes"""
    
    def __init__(self, model_name: str = "base"):
        self.model_name = model_name
        self.mode = self._detect_best_mode()
        self._setup_environment()
        
        logger.info(f"üöÄ Initialized Whisper engine in {self.mode} mode")
        logger.info(f"   Model: {model_name}")
        logger.info(f"   Environment: Docker container")
    
    def _detect_best_mode(self) -> str:
        """Detect the best available transcription mode"""
        
        # Mode 1: Try whisper.cpp SYCL (if available)
        if Path("/tmp/whisper.cpp/build_sycl/bin/whisper-cli").exists():
            return "sycl"
        
        # Mode 2: Try system whisper.cpp
        try:
            result = subprocess.run(["which", "whisper-cli"], capture_output=True)
            if result.returncode == 0:
                return "whisper_cpp_system"
        except:
            pass
        
        # Mode 3: Try OpenAI whisper
        try:
            import whisper
            return "openai_whisper"
        except ImportError:
            pass
        
        # Mode 4: Try whisperx 
        try:
            import whisperx
            return "whisperx"
        except ImportError:
            pass
        
        # Fallback: Mock mode for debugging
        logger.warning("‚ö†Ô∏è  No Whisper backend found, using mock mode")
        return "mock"
    
    def _setup_environment(self):
        """Set up environment for the selected mode"""
        if self.mode == "sycl":
            # Set up Intel oneAPI environment if available
            oneapi_env = "/opt/intel/oneapi/setvars.sh"
            if Path(oneapi_env).exists():
                # Source oneAPI environment
                result = subprocess.run([
                    "bash", "-c", 
                    f"source {oneapi_env} && env"
                ], capture_output=True, text=True)
                
                if result.returncode == 0:
                    for line in result.stdout.split('\n'):
                        if '=' in line:
                            key, value = line.split('=', 1)
                            os.environ[key] = value
                
                # Set Intel iGPU specific vars
                os.environ["ONEAPI_DEVICE_SELECTOR"] = "level_zero:gpu"
                os.environ["SYCL_DEVICE_FILTER"] = "gpu"
    
    def transcribe_file(self, audio_path: str, **kwargs) -> Dict:
        """Transcribe audio file using best available method"""
        
        start_time = time.time()
        duration = self._get_audio_duration(audio_path)
        
        logger.info(f"üéµ Transcribing {duration:.1f}s audio using {self.mode} mode")
        
        if self.mode == "sycl":
            result = self._transcribe_sycl(audio_path, **kwargs)
        elif self.mode == "whisper_cpp_system":
            result = self._transcribe_whisper_cpp(audio_path, **kwargs)
        elif self.mode == "openai_whisper":
            result = self._transcribe_openai_whisper(audio_path, **kwargs)
        elif self.mode == "whisperx":
            result = self._transcribe_whisperx(audio_path, **kwargs)
        else:
            result = self._transcribe_mock(audio_path, **kwargs)
        
        processing_time = time.time() - start_time
        rtf = processing_time / duration if duration > 0 else 0
        
        # Add performance info
        result.update({
            "duration": duration,
            "performance": {
                "total_time": f"{processing_time:.2f}s",
                "rtf": f"{1/rtf:.1f}x" if rtf > 0 else "‚àûx",
                "engine": f"Docker {self.mode.title()}",
                "model": self.model_name
            }
        })
        
        logger.info(f"‚úÖ Transcription complete: {processing_time:.2f}s ({1/rtf:.1f}x realtime)")
        
        return result
    
    def _transcribe_sycl(self, audio_path: str, **kwargs) -> Dict:
        """Transcribe using whisper.cpp SYCL"""
        sycl_binary = "/tmp/whisper.cpp/build_sycl/bin/whisper-cli"
        model_path = f"/tmp/whisper.cpp/models/ggml-{self.model_name}.bin"
        
        if not Path(model_path).exists():
            # Try to download model
            self._download_whisper_model(self.model_name)
        
        cmd = [
            sycl_binary,
            "-m", model_path, 
            "-f", audio_path,
            "-t", "4",
            "--print-progress"
        ]
        
        env = {**os.environ}
        env.update({
            "ONEAPI_DEVICE_SELECTOR": "level_zero:gpu",
            "SYCL_DEVICE_FILTER": "gpu"
        })
        
        result = subprocess.run(cmd, capture_output=True, text=True, env=env)
        
        if result.returncode != 0:
            logger.error(f"SYCL transcription failed: {result.stderr}")
            # Fall back to next available method
            return self._transcribe_fallback(audio_path, **kwargs)
        
        return self._parse_whisper_cpp_output(result.stdout)
    
    def _transcribe_whisper_cpp(self, audio_path: str, **kwargs) -> Dict:
        """Transcribe using system whisper.cpp"""
        cmd = ["whisper-cli", "-f", audio_path, "-t", "4"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            return self._transcribe_fallback(audio_path, **kwargs)
        
        return self._parse_whisper_cpp_output(result.stdout)
    
    def _transcribe_openai_whisper(self, audio_path: str, **kwargs) -> Dict:
        """Transcribe using OpenAI whisper"""
        try:
            import whisper
            model = whisper.load_model(self.model_name)
            result = model.transcribe(audio_path)
            
            return {
                "text": result["text"],
                "segments": result.get("segments", []),
                "language": result.get("language", "en")
            }
        except Exception as e:
            logger.error(f"OpenAI Whisper failed: {e}")
            return self._transcribe_mock(audio_path, **kwargs)
    
    def _transcribe_whisperx(self, audio_path: str, **kwargs) -> Dict:
        """Transcribe using WhisperX"""
        try:
            import whisperx
            import torch
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = whisperx.load_model(self.model_name, device)
            
            audio = whisperx.load_audio(audio_path)
            result = model.transcribe(audio)
            
            return {
                "text": result.get("text", ""),
                "segments": result.get("segments", []),
                "language": result.get("language", "en")
            }
        except Exception as e:
            logger.error(f"WhisperX failed: {e}")
            return self._transcribe_mock(audio_path, **kwargs)
    
    def _transcribe_mock(self, audio_path: str, **kwargs) -> Dict:
        """Mock transcription for debugging"""
        import time
        time.sleep(0.5)  # Simulate processing
        
        return {
            "text": "This is a mock transcription for debugging purposes. The actual Whisper backend is not available in this container.",
            "segments": [
                {
                    "start": 0.0,
                    "end": 5.0,
                    "text": "This is a mock transcription for debugging purposes."
                },
                {
                    "start": 5.0,
                    "end": 10.0,
                    "text": "The actual Whisper backend is not available in this container."
                }
            ],
            "language": "en"
        }
    
    def _transcribe_fallback(self, audio_path: str, **kwargs) -> Dict:
        """Fallback when primary method fails"""
        if self.mode != "mock":
            logger.warning(f"Falling back from {self.mode} to mock mode")
            return self._transcribe_mock(audio_path, **kwargs)
        return {"text": "Transcription failed", "segments": [], "language": "en"}
    
    def _parse_whisper_cpp_output(self, text_output: str) -> Dict:
        """Parse whisper.cpp output"""
        import re
        
        segments = []
        full_text = ""
        
        # Parse timestamped segments like [00:00:00.000 --> 00:00:04.640] text
        pattern = r'\[(\d{2}:\d{2}:\d{2}\.\d{3}) --> (\d{2}:\d{2}:\d{2}\.\d{3})\]\s+(.*)'
        
        for match in re.finditer(pattern, text_output):
            start_time = self._timestamp_to_seconds(match.group(1))
            end_time = self._timestamp_to_seconds(match.group(2))
            text = match.group(3).strip()
            
            segments.append({
                "start": start_time,
                "end": end_time,
                "text": text
            })
            
            full_text += text + " "
        
        if not segments and text_output.strip():
            full_text = text_output.strip()
            segments = [{"start": 0.0, "end": 0.0, "text": full_text}]
        
        return {
            "text": full_text.strip(),
            "segments": segments,
            "language": "en"
        }
    
    def _timestamp_to_seconds(self, timestamp: str) -> float:
        """Convert HH:MM:SS.mmm to seconds"""
        parts = timestamp.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = float(parts[2])
        return hours * 3600 + minutes * 60 + seconds
    
    def _get_audio_duration(self, audio_path: str) -> float:
        """Get audio duration using ffprobe"""
        try:
            cmd = [
                'ffprobe', '-v', 'quiet', 
                '-show_entries', 'format=duration',
                '-of', 'csv=p=0', audio_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                return float(result.stdout.strip())
        except:
            pass
        return 10.0  # Default fallback
    
    def _download_whisper_model(self, model_name: str):
        """Download Whisper model if not present"""
        models_dir = Path("/tmp/whisper.cpp/models")
        models_dir.mkdir(parents=True, exist_ok=True)
        
        # Try to download using whisper.cpp script if available
        download_script = Path("/tmp/whisper.cpp/models/download-ggml-model.sh")
        if download_script.exists():
            try:
                subprocess.run([
                    "bash", str(download_script), model_name
                ], cwd="/tmp/whisper.cpp", check=True, timeout=300)
            except:
                logger.warning(f"Failed to download {model_name} model")

def initialize_whisper_engine():
    """Initialize the Whisper engine"""
    global whisper_engine
    
    if whisper_engine is not None:
        return whisper_engine
    
    logger.info("üöÄ Initializing Docker-Ready Unicorn Amanuensis...")
    logger.info("=" * 60)
    
    try:
        whisper_engine = DockerWhisperEngine(DEFAULT_MODEL)
        logger.info("‚úÖ Engine initialized successfully")
        return whisper_engine
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize engine: {e}")
        # Create a minimal fallback engine
        whisper_engine = DockerWhisperEngine("base")
        return whisper_engine

def preprocess_audio(file_path: str) -> str:
    """Preprocess audio for transcription"""
    temp_wav = tempfile.mktemp(suffix='.wav')
    
    cmd = [
        'ffmpeg', '-i', file_path,
        '-ar', '16000',
        '-ac', '1', 
        '-c:a', 'pcm_s16le',
        '-y', temp_wav
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            return temp_wav
    except:
        pass
    
    # Return original if preprocessing fails
    return file_path

async def transcribe_audio(audio_path: str, model_name: str = "base", 
                          enable_diarization: bool = False) -> Dict:
    """Transcribe audio with the Docker-ready engine"""
    
    engine = initialize_whisper_engine()
    
    logger.info(f"üî• Starting transcription (model: {model_name})")
    start_time = time.time()
    
    try:
        # Preprocess audio
        logger.info("üéµ Preprocessing audio...")
        preprocessed_path = preprocess_audio(audio_path)
        
        # Transcribe
        result = engine.transcribe_file(
            preprocessed_path,
            model=model_name,
            diarization=enable_diarization
        )
        
        # Clean up temp file
        if preprocessed_path != audio_path and os.path.exists(preprocessed_path):
            os.unlink(preprocessed_path)
        
        total_time = time.time() - start_time
        
        logger.info(f"üöÄ Transcription completed in {total_time:.2f}s")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Transcription failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

# Routes
@app.get("/")
async def root():
    """API documentation"""
    engine = initialize_whisper_engine()
    return {
        "name": "ü¶Ñ Unicorn Amanuensis Production",
        "version": "2.0.0",
        "description": "Docker-ready transcription service",
        "engine_mode": engine.mode if engine else "unknown",
        "model": DEFAULT_MODEL,
        "endpoints": {
            "/": "API documentation",
            "/web": "Web interface",
            "/transcribe": "Main transcription endpoint",
            "/v1/audio/transcriptions": "OpenAI-compatible endpoint",
            "/status": "Server status",
            "/health": "Health check"
        }
    }

@app.get("/web", response_class=HTMLResponse)
async def web_ui():
    """Serve web interface"""
    index_path = static_dir / "index.html"
    if index_path.exists():
        return FileResponse(index_path)
    else:
        return HTMLResponse("""
        <h1>ü¶Ñ Unicorn Amanuensis</h1>
        <p>Docker Production Server</p>
        <p>Use the API endpoints to transcribe audio.</p>
        """)

@app.post("/transcribe")
async def transcribe_endpoint(
    file: UploadFile = File(None),
    url: str = Form(None),
    model: str = Form(DEFAULT_MODEL),
    diarization: bool = Form(False),
    response_format: str = Form("verbose_json")
):
    """Main transcription endpoint"""
    
    if not file and not url:
        raise HTTPException(status_code=400, detail="No audio file or URL provided")
    
    try:
        # Handle file upload or URL
        if file:
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{file.filename}") as temp_file:
                content = await file.read()
                temp_file.write(content)
                audio_path = temp_file.name
        else:
            # Download from URL
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                response.raise_for_status()
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_file.write(response.content)
                    audio_path = temp_file.name
        
        # Transcribe
        result = await transcribe_audio(audio_path, model, diarization)
        
        # Clean up
        if os.path.exists(audio_path):
            os.unlink(audio_path)
        
        if response_format == "text":
            return JSONResponse(content={"text": result["text"]})
        
        return JSONResponse(content=result)
        
    except Exception as e:
        logger.error(f"Transcription error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/audio/transcriptions")
async def openai_transcriptions(
    file: UploadFile = File(...),
    model: str = Form("whisper-1"),
    language: str = Form(None),
    prompt: str = Form(None),
    response_format: str = Form("json"),
    temperature: float = Form(0),
    timestamp_granularities: str = Form('["segment"]'),
    diarization: bool = Form(False)
):
    """OpenAI-compatible transcription endpoint"""
    
    # Map model names
    model_map = {
        "whisper-1": "base",
        "whisper-large": "large-v3",
        "whisper-large-v3": "large-v3"
    }
    
    actual_model = model_map.get(model, model)
    
    return await transcribe_endpoint(
        file=file,
        model=actual_model,
        diarization=diarization,
        response_format="verbose_json" if response_format != "text" else "text"
    )

@app.get("/status")
async def status():
    """Server status"""
    engine = initialize_whisper_engine()
    return {
        "status": "ready",
        "engine_mode": engine.mode if engine else "unknown",
        "model": DEFAULT_MODEL,
        "container": "docker",
        "version": "2.0.0"
    }

@app.get("/health")
async def health_check():
    """Health check"""
    try:
        engine = initialize_whisper_engine()
        return {
            "status": "healthy",
            "engine": engine.mode if engine else "unknown",
            "timestamp": time.time()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }

if __name__ == "__main__":
    import uvicorn
    
    logger.info("üöÄ Starting Docker-Ready Unicorn Amanuensis...")
    logger.info(f"üê≥ Container environment detected")
    logger.info(f"üåê Starting server on port {API_PORT}")
    
    # Pre-initialize engine
    initialize_whisper_engine()
    
    uvicorn.run(app, host="0.0.0.0", port=API_PORT, workers=1)