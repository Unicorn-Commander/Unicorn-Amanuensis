# Docker image for Unicorn Amanuensis with REAL AMD Phoenix NPU support
FROM python:3.13-slim

WORKDIR /app

# Install system dependencies including Boost for XRT
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    git \
    build-essential \
    libsndfile1 \
    ffmpeg \
    libboost-filesystem1.83.0 \
    libboost-program-options1.83.0 \
    libboost-system1.83.0 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements_npu.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements_npu.txt

# Install XRT Python bindings for AMD NPU
# Note: pyxrt will be mounted from host via docker-compose volume
# The XRT libs and pyxrt.so will be available from /opt/xilinx/xrt

# Add XRT library path (mounted from host)
ENV LD_LIBRARY_PATH=/opt/xilinx/xrt/lib:${LD_LIBRARY_PATH}
ENV PYTHONPATH=/opt/xilinx/xrt/python:${PYTHONPATH}

# Copy custom ONNX + direct IOCTL NPU runtime (SimplifiedNPURuntime)
COPY npu/ npu/

# Copy REAL working NPU server with SimplifiedNPURuntime + direct IOCTL
COPY server_whisperx_npu.py server.py

# Copy GUI files (templates and static with unicorn logo!)
COPY templates/ templates/
COPY static/ static/

# Create models directory
RUN mkdir -p /app/models

# NOTE: Models are mounted from host via docker-compose volume
# Run ./download-models.sh on the host before building to download:
#   - whisper-base-onnx-int8 (121 MB) - Fast model
#   - whisper-large-v3-onnx-int8 (1.4 GB) - Best accuracy
# Models will be available at /app/models/ in the container

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV WHISPER_MODEL_CACHE=/app/models
ENV WHISPER_MODEL=base
ENV WHISPER_NPU_MODEL_PATH=/app/models/whisper-base-onnx-int8
ENV API_PORT=9000

# Expose API port
EXPOSE 9000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:9000/health || exit 1

# Run server
CMD ["python", "server.py"]
