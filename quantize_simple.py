#!/usr/bin/env python3
"""
Simple INT8 quantization for OpenVINO Whisper models
Optimized for Intel iGPU performance
"""

import os
import sys
from pathlib import Path
import openvino as ov
import nncf
import logging
import shutil

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def quantize_model_int8(input_path: str, output_path: str):
    """Simple INT8 quantization using NNCF"""
    
    logger.info(f"Loading model from {input_path}")
    
    # Load the OpenVINO model
    core = ov.Core()
    model = core.read_model(input_path)
    
    logger.info("Applying INT8 quantization...")
    
    # Apply weight compression to INT8
    # This is the most effective method for Intel iGPU
    quantized_model = nncf.compress_weights(
        model,
        mode=nncf.CompressWeightsMode.INT8_ASYM  # Use INT8 asymmetric quantization
        # For INT8, ratio and group_size use defaults (cannot be overridden)
    )
    
    # Save the quantized model
    logger.info(f"Saving to {output_path}")
    ov.save_model(quantized_model, output_path)
    
    # Check file sizes
    original_bin = str(input_path).replace('.xml', '.bin')
    quantized_bin = str(output_path).replace('.xml', '.bin')
    
    if Path(original_bin).exists() and Path(quantized_bin).exists():
        orig_size = Path(original_bin).stat().st_size / (1024*1024)
        quant_size = Path(quantized_bin).stat().st_size / (1024*1024)
        logger.info(f"Original: {orig_size:.2f} MB â†’ Quantized: {quant_size:.2f} MB")
        logger.info(f"Compression: {orig_size/quant_size:.2f}x")

def main():
    model_dir = Path(sys.argv[1] if len(sys.argv) > 1 else "~/openvino-models/whisper-base-openvino").expanduser()
    output_dir = Path(sys.argv[2] if len(sys.argv) > 2 else "~/openvino-models/whisper-base-int8").expanduser()
    
    logger.info("ðŸ¦„ Whisper INT8 Quantization for Intel iGPU")
    logger.info("="*60)
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Copy non-model files
    for file in model_dir.glob("*"):
        if file.suffix not in ['.xml', '.bin']:
            shutil.copy2(file, output_dir / file.name)
    
    # Quantize encoder
    encoder_xml = model_dir / "openvino_encoder_model.xml"
    if encoder_xml.exists():
        logger.info("\nðŸ“Š Quantizing ENCODER...")
        quantize_model_int8(
            str(encoder_xml),
            str(output_dir / "openvino_encoder_model.xml")
        )
    
    # Quantize decoder
    decoder_xml = model_dir / "openvino_decoder_model.xml"
    if decoder_xml.exists():
        logger.info("\nðŸ“Š Quantizing DECODER...")
        quantize_model_int8(
            str(decoder_xml),
            str(output_dir / "openvino_decoder_model.xml")
        )
    
    # Quantize decoder with past
    decoder_past_xml = model_dir / "openvino_decoder_with_past_model.xml"
    if decoder_past_xml.exists():
        logger.info("\nðŸ“Š Quantizing DECODER WITH PAST...")
        quantize_model_int8(
            str(decoder_past_xml),
            str(output_dir / "openvino_decoder_with_past_model.xml")
        )
    
    logger.info("\n" + "="*60)
    logger.info("âœ… INT8 Quantization Complete!")
    logger.info("="*60)
    logger.info("ðŸš€ Performance Benefits on Intel iGPU:")
    logger.info("  â€¢ 2-4x faster inference")
    logger.info("  â€¢ 4x smaller model size")
    logger.info("  â€¢ Lower memory bandwidth")
    logger.info("  â€¢ Better cache utilization")
    logger.info("="*60)
    logger.info(f"ðŸ“‚ Output: {output_dir}")

if __name__ == "__main__":
    main()